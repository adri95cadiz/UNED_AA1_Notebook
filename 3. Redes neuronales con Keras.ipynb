{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 3: Introducción a las Redes Neuronales Artificiales con Keras\n",
    "\n",
    "## El perceptrón\n",
    "\n",
    "El primer elemento que debemos conocer es el perceptrón, una neurona que recibe una serie de entradas y genera una salida. La salida de la neurona se calcula como la suma ponderada de las entradas, más un término de sesgo, y se aplica una función de activación. La función de activación es una función no lineal que se aplica a la suma ponderada de las entradas. La función de activación más común es la función sigmoide, que se define como:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Scikit-learn provee con su propia clase para un perceptron, que implementa una red neuronal de una sola capa. Esta clase se encuentra en el módulo sklearn.linear_model. La clase Perceptron tiene los siguientes parámetros:\n",
    "\n",
    "* **penalty**: especifica el tipo de regularización que se aplica a los pesos. Puede ser None, l1 o l2.\n",
    "* **alpha**: especifica el valor de la regularización.\n",
    "* **fit_intercept**: especifica si se debe o no agregar un término de sesgo.\n",
    "* **max_iter**: especifica el número máximo de iteraciones.\n",
    "* **tol**: especifica la tolerancia para el criterio de parada.\n",
    "* **shuffle**: especifica si se debe o no barajar los datos antes de cada iteración.\n",
    "* **eta0**: especifica el valor del learning rate.\n",
    "* **n_jobs**: especifica el número de hilos a utilizar.\n",
    "\n",
    "Se puede usar tal cómo podríamos esperar, específicamente en el DataSet de IRIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(int) # Iris setosa?\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El perceptrón es un algoritmo de aprendizaje supervisado, por lo que necesita de un conjunto de datos de entrenamiento. El algoritmo de aprendizaje se basa en el algoritmo de descenso de gradiente, que consiste en actualizar los pesos de la red neuronal de forma iterativa, de forma que se minimice la función de coste. La función de coste que se utiliza en el perceptrón es la función de coste logarítmica, que se define como:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "donde $h_\\theta(x^{(i)})$ es la salida de la red neuronal para la entrada $x^{(i)}$ y $\\theta$ son los pesos de la red neuronal. El algoritmo de descenso de gradiente consiste en repetir el siguiente proceso hasta que se cumpla el criterio de parada:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$$\n",
    "\n",
    "donde $\\alpha$ es el learning rate. El algoritmo de descenso de gradiente se puede implementar de forma vectorizada, de forma que se actualicen todos los pesos de la red neuronal en cada iteración:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha\\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "donde $\\nabla_\\theta J(\\theta)$ es el gradiente de la función de coste. El gradiente de la función de coste se puede calcular de forma vectorizada como:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\frac{1}{m}X^T(h_\\theta(X) - y)$$\n",
    "\n",
    "donde $X$ es la matriz de datos de entrenamiento, $y$ es el vector de etiquetas de entrenamiento y $h_\\theta(X)$ es el vector de salidas de la red neuronal para los datos de entrenamiento.\n",
    "\n",
    "El algoritmo de descenso de gradiente es un algoritmo iterativo, por lo que se puede detener cuando se cumpla el criterio de parada. El criterio de parada más común es que el algoritmo se detenga cuando el valor de la función de coste no cambie más de un umbral. El algoritmo de descenso de gradiente también puede detenerse cuando se alcanza un número máximo de iteraciones. El algoritmo de descenso de gradiente también puede detenerse cuando el valor de la función de coste es menor que un umbral.\n",
    "\n",
    "El algoritmo de descenso de gradiente puede converger a un mínimo local, por lo que es posible que el algoritmo no encuentre el mínimo global. Para evitar este problema, se puede utilizar el algoritmo de descenso de gradiente estocástico, que consiste en actualizar los pesos de la red neuronal de forma iterativa, de forma que se minimice la función de coste, pero utilizando un solo ejemplo de entrenamiento en cada iteración. El algoritmo de descenso de gradiente estocástico se puede implementar de forma vectorizada, de forma que se actualicen todos los pesos de la red neuronal en cada iteración:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha\\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "donde $\\nabla_\\theta J(\\theta)$ es el gradiente de la función de coste. El gradiente de la función de coste se puede calcular de forma vectorizada como:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = (h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
    "\n",
    "donde $x^{(i)}$ es el ejemplo de entrenamiento y $y^{(i)}$ es la etiqueta del ejemplo de entrenamiento. El algoritmo de descenso de gradiente estocástico es un algoritmo iterativo, por lo que se puede detener cuando se cumpla el criterio de parada. El criterio de parada más común es que el algoritmo se detenga cuando el valor de la función de coste no cambie más de un umbral. El algoritmo de descenso de gradiente estocástico también puede detenerse cuando se alcanza un número máximo de iteraciones. El algoritmo de descenso de gradiente estocástico también puede detenerse cuando el valor de la función de coste es menor que un umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón multicapa\n",
    "\n",
    "Se encontraron varias debilidades en el perceptrón, como la incapacidad de resolver problemas no lineales. En 1957, el matemático Frank Rosenblatt publicó un artículo en el que propuso una modificación del perceptrón, que se conoce como perceptrón multicapa. El perceptrón multicapa es un algoritmo de aprendizaje supervisado, por lo que necesita de un conjunto de datos de entrenamiento. El algoritmo de aprendizaje se basa en el algoritmo de descenso de gradiente, que consiste en actualizar los pesos de la red neuronal de forma iterativa, de forma que se minimice la función de coste.\n",
    "\n",
    "Un MLP está compuesto por varias capas de neuronas. La primera capa de neuronas se conoce como capa de entrada, y se utiliza para introducir los datos de entrada. La última capa de neuronas se conoce como capa de salida, y se utiliza para obtener las salidas de la red neuronal. Las capas intermedias se conocen como capas ocultas, y se utilizan para calcular las salidas de la red neuronal. La red neuronal se entrena de forma iterativa, de forma que se minimice la función de coste. En cada iteración, se calculan las salidas de la red neuronal para los datos de entrenamiento, y se actualizan los pesos de la red neuronal de forma que se minimice la función de coste. \n",
    "\n",
    "Para entrenar el perceptrón multicapa, se utiliza el método backpropagation, que consiste en propagar hacia atrás los errores de la red neuronal, de forma que se actualicen los pesos de la red neuronal de forma que se minimice la función de coste. El método backpropagation se puede implementar de forma vectorizada, de forma que se actualicen todos los pesos de la red neuronal en cada iteración:\n",
    "\n",
    "$$\\theta := \\theta - \\alpha\\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "donde $\\nabla_\\theta J(\\theta)$ es el gradiente de la función de coste. El gradiente de la función de coste se puede calcular de forma vectorizada como:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\frac{1}{m}X^T(h_\\theta(X) - y)$$\n",
    "\n",
    "donde $X$ es la matriz de datos de entrenamiento, $y$ es el vector de etiquetas de entrenamiento y $h_\\theta(X)$ es el vector de salidas de la red neuronal para los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP con tensorflow y keras\n",
    "\n",
    "Tensorflow es una biblioteca de código abierto para el aprendizaje automático, que se puede utilizar para crear y entrenar redes neuronales. Keras es una biblioteca de código abierto para el aprendizaje automático, que se puede utilizar para crear y entrenar redes neuronales. Keras se puede utilizar con tensorflow, de forma que se pueden crear y entrenar redes neuronales con tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Hemos instalado TensorFlow y Keras, veamos su versión\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construyendo un clasificador de imágenes con tensorflow y keras\n",
    "\n",
    "Vamos a utilizar fashion_mnist, que es un conjunto de datos de imágenes de ropa, para construir un clasificador de imágenes con tensorflow y keras. El conjunto de datos fashion_mnist contiene 60.000 imágenes de entrenamiento y 10.000 imágenes de prueba. Cada imagen es una imagen de 28x28 píxeles en escala de grises, y cada imagen está etiquetada con una de las siguientes 10 clases:\n",
    "\n",
    "* Camiseta/top\n",
    "* Pantalón\n",
    "* Suéter\n",
    "* Vestido\n",
    "* Abrigo\n",
    "* Sandalia\n",
    "* Camisa\n",
    "* Zapatilla deportiva\n",
    "* Bolso\n",
    "* Bota\n",
    "\n",
    "### Cargando el conjunto de datos con keras\n",
    "\n",
    "El conjunto de datos fashion_mnist se puede cargar con keras, de forma que se obtiene un conjunto de datos de entrenamiento y un conjunto de datos de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una diferencia importante al cargar el dataset con keras es que las imágenes se representan como un array de 28x28 píxeles, y las etiquetas se representan como un array de 10 elementos, donde cada elemento es 0, excepto el elemento correspondiente a la clase de la imagen, que es 1.\n",
    "\n",
    "Veamos la forma de los arrays de imágenes y etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset está dividido en entrenamiento y prueba, pero no hay conjunto de validación, por lo que vamos a crear uno. Además, vamos a escalar las imágenes, de forma que los valores de los píxeles estén entre 0 y 1, en lugar de entre 0 y 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En MNIST, las imágenes están etiquetadas con un número del 0 al 9, pero en fashion_mnist, las imágenes están etiquetadas con una clase, por lo que vamos a convertir las etiquetas de números a etiquetas de clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el Modelo usando la API secuencial de keras\n",
    "\n",
    "Vamos a crear un MLP de clasificación con dos capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendamos este código línea por línea:\n",
    "\n",
    "* La primera línea crea un modelo secuencial, que es un modelo de keras que se utiliza para crear redes neuronales. El modelo secuencial es una pila lineal de capas, y cada capa tiene exactamente una entrada y una salida. El modelo secuencial se puede utilizar para crear redes neuronales feedforward, como MLPs, CNNs y RNNs.\n",
    "* La segunda línea crea una capa de entrada, que es una capa de keras que se utiliza para introducir los datos de entrada. La capa de entrada tiene 28x28=784 neuronas, y cada neurona recibe un valor de entrada de 1. La capa de entrada no tiene pesos, y no se entrena.\n",
    "* La tercera línea crea una capa oculta, que es una capa de keras que se utiliza para calcular las salidas de la red neuronal. La capa oculta tiene 300 neuronas, y cada neurona recibe un valor de entrada de 784. La capa oculta tiene 784x300=235.200 pesos, y se entrena, utiliza la función de activación ReLU, y se inicializan los pesos con la distribución normal estándar.\n",
    "* La cuarta línea crea una capa oculta, que es una capa de keras que se utiliza para calcular las salidas de la red neuronal. La capa oculta tiene 100 neuronas, y cada neurona recibe un valor de entrada de 300. La capa oculta tiene 300x100=30.000 pesos, y se entrena.\n",
    "* La quinta línea crea una capa de salida, que es una capa de keras que se utiliza para calcular las salidas de la red neuronal. La capa de salida tiene 10 neuronas, y cada neurona recibe un valor de entrada de 100. La capa de salida tiene 100x10=1.000 pesos, y se entrena, utiliza la función de activación softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También, en lugar de ir añadiendo las capas una a una, podemos hacerlo en una lista al inicializar el modelo\n",
    "\n",
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.Dense(300, activation=\"relu\"),\n",
    " keras.layers.Dense(100, activation=\"relu\"),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método summary() muestra un resumen del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener una lista de todas las capas del modelo, de forma que podemos acceder a sus parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x1ab983eba30>,\n",
       " <keras.layers.core.dense.Dense at 0x1ab98333a00>,\n",
       " <keras.layers.core.dense.Dense at 0x1ab98330940>,\n",
       " <keras.layers.core.dense.Dense at 0x1ab9576b070>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros de una capa se pueden acceder con el método get_weights() y el método set_weights(), en una capa densa, los parámetros son los pesos y los sesgos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06241401,  0.02100148,  0.02958135, ...,  0.06821564,\n",
       "         0.04761449,  0.06490973],\n",
       "       [ 0.0015179 ,  0.07062288,  0.05538411, ...,  0.0279075 ,\n",
       "        -0.0367073 ,  0.038169  ],\n",
       "       [ 0.06145817,  0.0665262 ,  0.01647068, ...,  0.01668037,\n",
       "         0.0175985 ,  0.06388563],\n",
       "       ...,\n",
       "       [-0.04364371, -0.00500527,  0.05191055, ..., -0.01805245,\n",
       "         0.03998069,  0.04848472],\n",
       "       [-0.06898757,  0.06849687,  0.06011496, ...,  0.06589341,\n",
       "        -0.07380725, -0.05731675],\n",
       "       [-0.03863191,  0.00113554, -0.01719293, ..., -0.00583737,\n",
       "         0.04682643,  0.06107487]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa densa se inicializa con pesos aleatorios, y los sesgos con ceros, si queremos usar otros valores, podemos asignar el valor de los parámetros:\n",
    "\n",
    "* kernel_initializer: inicializador de los pesos.\n",
    "* bias_initializer: inicializador de los sesgos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilando el modelo\n",
    "\n",
    "Después de crear el modelo, tenemos que compilarlo, para ello, tenemos que especificar:\n",
    "\n",
    "* La función de pérdida: la función de pérdida se utiliza para medir qué tan bien el modelo se ajusta a los datos de entrenamiento. La función de pérdida se minimiza durante el entrenamiento.\n",
    "* El optimizador: el optimizador se utiliza para ajustar los parámetros del modelo para minimizar la función de pérdida.\n",
    "* Las métricas: las métricas se utilizan para monitorear el entrenamiento y la evaluación del modelo. En este ejemplo, vamos a utilizar la precisión como métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    " optimizer=\"sgd\",\n",
    " metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explicar el código línea por línea:\n",
    "\n",
    "* La primera línea compila el modelo, para ello, se especifica la función de pérdida, el optimizador y las métricas.\n",
    "* Usamos la función de pérdida sparse_categorical_crossentropy, que es la función de pérdida que se utiliza para problemas de clasificación multiclase, donde las etiquetas son enteros. Si en lugar de enteros, las etiquetas fueran vectores de 10 elementos, donde cada elemento es 0, excepto el elemento correspondiente a la clase de la imagen, que es 1, entonces tendríamos que usar la función de pérdida categorical_crossentropy. Si estuviéramos resolviendo un problema de clasificación binaria, donde las etiquetas son 0 o 1, entonces tendríamos que usar la función de pérdida binary_crossentropy.\n",
    "* En cuanto al optimizador, \"sdg\" es un optimizador de descenso de gradiente estocástico, que es un optimizador que se utiliza para minimizar la función de pérdida. El optimizador se puede configurar con varios parámetros, como la tasa de aprendizaje, el momento, etc. En este ejemplo, vamos a utilizar la tasa de aprendizaje por defecto, que es 0.01.\n",
    "* Finalmente, ya que estamos resolviendo un problema de clasificación multiclase, vamos a utilizar la precisión como métrica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando y evaluando el modelo\n",
    "\n",
    "El modelo está listo para ser entrenado, para ello, tenemos que llamar al método fit(), que recibe los datos de entrenamiento, el número de épocas y el tamaño del lote. En este ejemplo, vamos a entrenar el modelo durante 30 épocas, utilizando lotes de 32 imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7138 - accuracy: 0.7650 - val_loss: 0.5114 - val_accuracy: 0.8292\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4853 - accuracy: 0.8303 - val_loss: 0.4614 - val_accuracy: 0.8400\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4408 - accuracy: 0.8452 - val_loss: 0.4286 - val_accuracy: 0.8496\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4122 - accuracy: 0.8556 - val_loss: 0.4086 - val_accuracy: 0.8608\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3921 - accuracy: 0.8629 - val_loss: 0.3765 - val_accuracy: 0.8720\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3754 - accuracy: 0.8670 - val_loss: 0.3750 - val_accuracy: 0.8740\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3630 - accuracy: 0.8721 - val_loss: 0.3652 - val_accuracy: 0.8750\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3515 - accuracy: 0.8759 - val_loss: 0.3446 - val_accuracy: 0.8784\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3395 - accuracy: 0.8793 - val_loss: 0.3870 - val_accuracy: 0.8620\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3326 - accuracy: 0.8805 - val_loss: 0.3445 - val_accuracy: 0.8734\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3237 - accuracy: 0.8851 - val_loss: 0.3507 - val_accuracy: 0.8766\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3165 - accuracy: 0.8874 - val_loss: 0.3341 - val_accuracy: 0.8810\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3083 - accuracy: 0.8893 - val_loss: 0.3257 - val_accuracy: 0.8818\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3027 - accuracy: 0.8905 - val_loss: 0.3442 - val_accuracy: 0.8800\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2948 - accuracy: 0.8943 - val_loss: 0.3145 - val_accuracy: 0.8862\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2884 - accuracy: 0.8953 - val_loss: 0.3195 - val_accuracy: 0.8874\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2839 - accuracy: 0.8978 - val_loss: 0.3190 - val_accuracy: 0.8870\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2789 - accuracy: 0.8994 - val_loss: 0.3234 - val_accuracy: 0.8846\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2729 - accuracy: 0.9023 - val_loss: 0.3042 - val_accuracy: 0.8920\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2678 - accuracy: 0.9029 - val_loss: 0.3065 - val_accuracy: 0.8914\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2640 - accuracy: 0.9050 - val_loss: 0.3183 - val_accuracy: 0.8824\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2579 - accuracy: 0.9067 - val_loss: 0.3198 - val_accuracy: 0.8822\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2539 - accuracy: 0.9089 - val_loss: 0.3043 - val_accuracy: 0.8922\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2505 - accuracy: 0.9104 - val_loss: 0.2998 - val_accuracy: 0.8914\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2457 - accuracy: 0.9117 - val_loss: 0.2992 - val_accuracy: 0.8942\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2415 - accuracy: 0.9129 - val_loss: 0.2885 - val_accuracy: 0.8964\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2371 - accuracy: 0.9148 - val_loss: 0.3116 - val_accuracy: 0.8912\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2330 - accuracy: 0.9164 - val_loss: 0.2982 - val_accuracy: 0.8920\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2299 - accuracy: 0.9165 - val_loss: 0.2971 - val_accuracy: 0.8974\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2252 - accuracy: 0.9201 - val_loss: 0.2900 - val_accuracy: 0.8966\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    " validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pasamos las características de entrada (X_train), y las clases objetivo (y_train), así como el número de épocas a entrenar (por defecto, sólo sería 1). También pasamos un conjunto de validación (esto es opcional). Si se especifica, el modelo evaluará la función de pérdida y las métricas en este conjunto al final de cada época, y mostrará los resultados. En este ejemplo, vamos a utilizar el conjunto de validación para ver cómo evoluciona el modelo durante el entrenamiento.\n",
    "\n",
    "Si hay mucha diferencia entre la función de pérdida y las métricas en el conjunto de entrenamiento y en el conjunto de validación, entonces el modelo está sobreajustando los datos de entrenamiento. \n",
    "\n",
    "Si el entrenamiento fue muy sesgado, es decir, si la función de pérdida y las métricas en el conjunto de entrenamiento son muy altas, y en el conjunto de validación son muy bajas, entonces el modelo no está aprendiendo nada. Para evitar este problema, podemos utilizar el argumento class_weight, que se utiliza para asignar pesos a las clases, de forma que el modelo se enfoca en aprender las clases que tienen un peso mayor. \n",
    "\n",
    "Asignando el valor de la variable history a la llamada al método fit(), podemos acceder a los valores de la función de pérdida y las métricas en el conjunto de entrenamiento y en el conjunto de validación, durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que la función de pérdida y la precisión en el conjunto de entrenamiento aumentan, mientras que la función de pérdida y la precisión en el conjunto de validación disminuyen, lo que indica que el modelo está aprendiendo.\n",
    "\n",
    "Si no estamos satisfechos con el rendimiento del modelo, podemos entrenarlo durante más épocas, o utilizar un modelo más complejo, o utilizar más datos de entrenamiento. Y como Keras continúa el entrenamiento desde donde lo dejamos, podemos llamar al método fit() varias veces, para continuar el entrenamiento.\n",
    "\n",
    "El conjunto de validación es un conjunto de datos que no se utiliza durante el entrenamiento, por lo que podemos utilizarlo para evaluar el modelo. Para ello, llamamos al método evaluate(), que recibe los datos de entrada y las clases objetivo, y devuelve la función de pérdida y las métricas en el conjunto de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 757us/step - loss: 76.2380 - accuracy: 0.8248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[76.23802947998047, 0.8248000144958496]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando el modelo para hacer predicciones\n",
    "\n",
    "Una vez que el modelo está entrenado, podemos utilizarlo para hacer predicciones. Para ello, llamamos al método predict(), que recibe los datos de entrada, y devuelve las predicciones del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como no tenemos nuevas instancias, vamos a usar las de test para predecir\n",
    "\n",
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con cada instancia, el modelo devuelve un vector de 10 elementos, donde cada elemento es la probabilidad de que la instancia pertenezca a la clase correspondiente. Para obtener la clase predicha, podemos utilizar la función argmax() de numpy, que devuelve el índice del elemento máximo de un vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predecir clase usando argmax de numpy\n",
    "import numpy as np\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizando un MLP de regresión utilizando la API secuencial\n",
    "\n",
    "Vamos a cambiar al problema de regresión, para ello, vamos a utilizar el conjunto de datos de California Housing, que contiene información sobre las casas de California.\n",
    "\n",
    "Usaremos la función fetch_california_housing() de Scikit-Learn para descargar el conjunto de datos. Esta función devuelve un diccionario con los datos, y también devuelve una descripción del conjunto de datos. Este conjunto de datos es más simple que el anterior, ya que no contiene valores perdidos, y todas las características son numéricas. Además, no hay ninguna característica categórica, por lo que no tenemos que realizar ninguna transformación de características.\n",
    "\n",
    "Tras cargar los datos, vamos a dividirlos en un conjunto de entrenamiento y un conjunto de test y vamos a normalizarlos. Para ello, vamos a utilizar la clase StandardScaler de Scikit-Learn, que calcula la media y la desviación estándar de los datos, y luego los normaliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    " housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    " X_train_full, y_train_full)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar la API secuencial de Keras para construir un modelo de regresión es muy similar a la construcción de un modelo de clasificación. La única diferencia es que en lugar de utilizar la función softmax() en la última capa, vamos a utilizar la función de activación lineal, que no realiza ninguna transformación. Además, en lugar de utilizar la función de pérdida crossentropy, vamos a utilizar la función de pérdida MSE (Mean Squared Error), que es la función de pérdida más común para problemas de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.8309 - val_loss: 0.6793\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 649us/step - loss: 1.0034 - val_loss: 0.4832\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.4947 - val_loss: 0.4509\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 629us/step - loss: 0.4559 - val_loss: 0.4150\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.4362 - val_loss: 0.4013\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 594us/step - loss: 0.4234 - val_loss: 0.3881\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.4134 - val_loss: 0.3835\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.4074 - val_loss: 0.3768\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.4077 - val_loss: 0.4056\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.4063 - val_loss: 0.3737\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3973 - val_loss: 0.3703\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3907 - val_loss: 0.3666\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.3939 - val_loss: 0.3728\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 604us/step - loss: 0.3843 - val_loss: 0.3617\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 644us/step - loss: 0.3802 - val_loss: 0.3622\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.3797 - val_loss: 0.3573\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.3752 - val_loss: 0.3678\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.3745 - val_loss: 0.3749\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.3720 - val_loss: 0.3528\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.3690 - val_loss: 0.3534\n",
      "162/162 [==============================] - 0s 534us/step - loss: 0.3882\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    " keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    " keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    " validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7467659],\n",
       "       [1.133008 ],\n",
       "       [1.6349635]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo modelos complejos utilizando la API funcional\n",
    "\n",
    "Los modelos secuenciales son bastante comunes, pero no son los únicos. En ocasiones, necesitamos construir modelos más complejos, como modelos con múltiples entradas o múltiples salidas, o modelos con capas compartidas. Para ello, podemos utilizar la API funcional de Keras.\n",
    "\n",
    "Un ejemplo de un modelo funcional sería el modelo Wide & Deep, que se utiliza para problemas de regresión y clasificación. Este modelo combina dos tipos de entradas: una entrada de características profundas, que se conecta a una red neuronal profunda, y una entrada de características anchas, que se conecta directamente a la salida.\n",
    "\n",
    "Esta arquitectura permite que el modelo aprenda características profundas y características simples, y luego las combine para hacer predicciones. Por ejemplo, el modelo podría aprender características profundas de las imágenes, y luego combinarlas con características simples, como el precio de la casa, para hacer predicciones.\n",
    "\n",
    "Vamos a construir un modelo Wide & Deep para el problema de regresión de California Housing. Para ello, vamos a utilizar la API funcional de Keras. En primer lugar, vamos a crear una entrada para las características profundas, y vamos a conectarla a una red neuronal profunda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explicar cada línea de código:\n",
    "\n",
    "* Creamos una entrada para las características profundas, utilizando la clase Input() de Keras. Esta clase recibe el tamaño de la entrada, y devuelve un objeto que representa la entrada.\n",
    "\n",
    "* Creamos una capa oculta densa con 30 neuronas, utilizando la función de activación ReLU, y conectamos la entrada a esta capa oculta.\n",
    "\n",
    "* Creamos una segunda capa oculta densa con 30 neuronas, utilizando la función de activación ReLU, y conectamos la salida de la primera capa oculta a esta capa oculta.\n",
    "\n",
    "* Creamos una capa concatenada, utilizando la clase Concatenate() de Keras. Esta clase recibe una lista de capas, y devuelve una capa que concatena las salidas de las capas de entrada.\n",
    "\n",
    "* Creamos una capa de salida densa con una sola neurona, y conectamos la salida de la capa concatenada a esta capa de salida.\n",
    "\n",
    "* Creamos un modelo, utilizando la clase Model() de Keras. Esta clase recibe la entrada y la salida, y devuelve un modelo que recibe las características profundas como entrada, y devuelve las salidas de la capa de salida.\n",
    "\n",
    "\n",
    "Una vez construido el modelo, podemos compilarlo y entrenarlo de la misma forma que hemos hecho con los modelos secuenciales.\n",
    "\n",
    "Pero y si queremos que un subgrupo de capas se conecte a otra entrada? Por ejemplo, si queremos que la capa de salida se conecte a la entrada de características anchas, en lugar de a la salida de la capa oculta? Para ello, podemos utilizar la clase Input() de Keras para crear una segunda entrada, y luego conectar la capa de salida a esta segunda entrada. Supongamos que queremos enviar 5 características por el camino ancho, y 6 características por el camino profundo. Podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos compilar el modelo como normalmente, pero al usar el método fit() de Keras, debemos pasar las características anchas y profundas como dos matrices separadas. X_train_A y X_train_B, lo mismo sucede para X_valid y para X_test; así como para X_new al llamar a evaluate() y predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/363 [..............................] - ETA: 52s - loss: 4.5481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdriánPortilloSánche\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 856us/step - loss: 2.2922 - val_loss: 1.0299\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.8720 - val_loss: 0.7264\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.7086 - val_loss: 0.6394\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.6482 - val_loss: 0.5960\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.6167 - val_loss: 0.5739\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.5983 - val_loss: 0.5576\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.5834 - val_loss: 0.5407\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 720us/step - loss: 0.5718 - val_loss: 0.5276\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.5571 - val_loss: 0.5149\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.5492 - val_loss: 0.5060\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.5419 - val_loss: 0.4988\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.5333 - val_loss: 0.4925\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.5290 - val_loss: 0.4908\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.5227 - val_loss: 0.4820\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.5173 - val_loss: 0.4788\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.5114 - val_loss: 0.4731\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.5059 - val_loss: 0.4704\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.5033 - val_loss: 0.4679\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 745us/step - loss: 0.4958 - val_loss: 0.4592\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.4957 - val_loss: 0.4559\n",
      "162/162 [==============================] - 0s 440us/step - loss: 0.4820\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    " validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchos casos en los que podríamos querer construir modelos complejos, como modelos con múltiples entradas y múltiples salidas, o modelos con capas compartidas. La API funcional de Keras es muy útil para construir modelos complejos, y es muy fácil de usar.\n",
    "\n",
    "Añadir más salidas es tan fácil como añadir más capas de salida. Por ejemplo, supongamos que queremos añadir una capa de salida para clasificar las imágenes en 5 clases. Podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada capa de salida requiere su propia función de pérdida, así que debemos pasar una lista de funciones de pérdida al compilar el modelo. Por ejemplo, si queremos que la capa de salida de regresión utilice la función de pérdida MSE, y la capa de salida de clasificación utilice la función de pérdida Stochastic Gradient Descent, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cuando entrenamos el modelo, debemos pasar etiquetas para ambas salidas. En este ejemplo usaremos las mismas etiquetas para ambas salidas, pero en la práctica, es muy probable que tengamos etiquetas diferentes para cada salida. Por ejemplo, podríamos tener una etiqueta de precio para la salida de regresión, y una etiqueta de clase para la salida de clasificación. Por ello pasamos y_train e y_valid como una lista de dos matrices, [y_train, y_train], [y_valid, y_valid]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 977us/step - loss: 0.8424 - main_output_loss: 0.6977 - aux_output_loss: 2.1450 - val_loss: 0.8931 - val_main_output_loss: 0.8680 - val_aux_output_loss: 1.1191\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 1.1455 - main_output_loss: 1.1447 - aux_output_loss: 1.1533 - val_loss: 0.6233 - val_main_output_loss: 0.5619 - val_aux_output_loss: 1.1753\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6128 - main_output_loss: 0.5584 - aux_output_loss: 1.1026 - val_loss: 0.6019 - val_main_output_loss: 0.5605 - val_aux_output_loss: 0.9740\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.6832 - main_output_loss: 0.6572 - aux_output_loss: 0.9178 - val_loss: 0.9647 - val_main_output_loss: 0.9799 - val_aux_output_loss: 0.8280\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 751us/step - loss: 1.7713 - main_output_loss: 1.8633 - aux_output_loss: 0.9435 - val_loss: 0.8063 - val_main_output_loss: 0.7989 - val_aux_output_loss: 0.8728\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.6589 - main_output_loss: 0.6361 - aux_output_loss: 0.8638 - val_loss: 0.4587 - val_main_output_loss: 0.4216 - val_aux_output_loss: 0.7926\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.6395 - main_output_loss: 0.6215 - aux_output_loss: 0.8012 - val_loss: 0.4384 - val_main_output_loss: 0.4043 - val_aux_output_loss: 0.7455\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 753us/step - loss: 0.5359 - main_output_loss: 0.5115 - aux_output_loss: 0.7546 - val_loss: 0.4299 - val_main_output_loss: 0.3972 - val_aux_output_loss: 0.7244\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.4750 - main_output_loss: 0.4462 - aux_output_loss: 0.7341 - val_loss: 0.4141 - val_main_output_loss: 0.3848 - val_aux_output_loss: 0.6778\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.4611 - main_output_loss: 0.4356 - aux_output_loss: 0.6912 - val_loss: 0.4065 - val_main_output_loss: 0.3789 - val_aux_output_loss: 0.6542\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.4289 - main_output_loss: 0.4029 - aux_output_loss: 0.6634 - val_loss: 0.3992 - val_main_output_loss: 0.3738 - val_aux_output_loss: 0.6282\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.4186 - main_output_loss: 0.3939 - aux_output_loss: 0.6407 - val_loss: 0.4018 - val_main_output_loss: 0.3784 - val_aux_output_loss: 0.6120\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 762us/step - loss: 0.4443 - main_output_loss: 0.4246 - aux_output_loss: 0.6214 - val_loss: 0.3949 - val_main_output_loss: 0.3725 - val_aux_output_loss: 0.5959\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.4037 - main_output_loss: 0.3814 - aux_output_loss: 0.6049 - val_loss: 0.3945 - val_main_output_loss: 0.3743 - val_aux_output_loss: 0.5765\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.4070 - main_output_loss: 0.3865 - aux_output_loss: 0.5920 - val_loss: 0.3876 - val_main_output_loss: 0.3673 - val_aux_output_loss: 0.5702\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3997 - main_output_loss: 0.3795 - aux_output_loss: 0.5819 - val_loss: 0.3814 - val_main_output_loss: 0.3621 - val_aux_output_loss: 0.5544\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 739us/step - loss: 0.3929 - main_output_loss: 0.3733 - aux_output_loss: 0.5697 - val_loss: 0.3823 - val_main_output_loss: 0.3642 - val_aux_output_loss: 0.5449\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3907 - main_output_loss: 0.3723 - aux_output_loss: 0.5563 - val_loss: 0.3735 - val_main_output_loss: 0.3559 - val_aux_output_loss: 0.5321\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3815 - main_output_loss: 0.3632 - aux_output_loss: 0.5464 - val_loss: 0.3756 - val_main_output_loss: 0.3588 - val_aux_output_loss: 0.5265\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.3856 - main_output_loss: 0.3686 - aux_output_loss: 0.5383 - val_loss: 0.4412 - val_main_output_loss: 0.4327 - val_aux_output_loss: 0.5175\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    " [X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    " validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando evaluamos el modelo, Keras devuelve el valor de la función de pérdida para cada salida, y el valor total de la función de pérdida. Por ejemplo, si queremos evaluar el modelo en el conjunto de prueba, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 497us/step - loss: 0.4312 - main_output_loss: 0.4227 - aux_output_loss: 0.5082\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    " [X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual manera, el método predict() de Keras devuelve una matriz por cada salida. Por ejemplo, si queremos hacer predicciones para el conjunto de prueba, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos construir cualquier arquitectura de red neuronal que queramos, utilizando la API funcional de Keras. Vamos a ver una última forma de construir modelos complejos, utilizando la API de subclases de Keras.\n",
    "\n",
    "## Utilizando la API de subclases para construir modelos dinámicos\n",
    "\n",
    "Las APIs secuencial y funcional de Keras son muy útiles para construir modelos estáticos, pero no son tan útiles para construir modelos dinámicos. Por ejemplo, si queremos construir un modelo que cambia su arquitectura en función de los datos de entrada, no podemos hacerlo con la API secuencial o funcional de Keras. Para construir modelos dinámicos, podemos utilizar la API de subclases de Keras.\n",
    "\n",
    "Simplemente tenemos que crear una clase que herede de la clase Model de Keras, y definir los métodos __init__() y call(). El método __init__() se utiliza para crear las capas del modelo, y el método call() se utiliza para definir la arquitectura del modelo. Por ejemplo, supongamos que queremos construir un modelo que recibe una matriz de características, y devuelve una matriz de predicciones. Podemos hacerlo de la siguiente forma, con el ejemplo anterior de modelo Wide & Deep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    \n",
    " def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "    super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "    self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "    self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "    self.main_output = keras.layers.Dense(1)\n",
    "    self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    " def call(self, inputs):\n",
    "    input_A, input_B = inputs\n",
    "    hidden1 = self.hidden1(input_B)\n",
    "    hidden2 = self.hidden2(hidden1)\n",
    "    concat = keras.layers.concatenate([input_A, hidden2])\n",
    "    main_output = self.main_output(concat)\n",
    "    aux_output = self.aux_output(hidden2)\n",
    "    return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo se parece mucho al ejemplo de la API funcional, pero hay una diferencia importante: en lugar de crear una instancia de la clase Model, simplemente creamos una instancia de la clase WideAndDeepModel. Esto es posible porque la clase WideAndDeepModel hereda de la clase Model de Keras. Cuando llamamos al método fit() de Keras, Keras crea una instancia de la clase WideAndDeepModel, y luego llama al método call() de la instancia para construir el grafo de cálculo. El método call() se llama automáticamente cuando llamamos al método fit(), evaluate() o predict() de Keras.\n",
    "\n",
    "Esto viene con un coste, la arquitectura del modelo está oculta al método call(), por lo que Keras no puede inspeccionarla, ni guardarla, ni mostrarla. Si queremos ver la arquitectura del modelo, podemos llamar al método summary() de Keras, o podemos imprimir el modelo como una cadena de texto, como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.2409 - output_1_loss: 1.1207 - output_2_loss: 2.3221 - val_loss: 0.6052 - val_output_1_loss: 0.5218 - val_output_2_loss: 1.3556\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.6236 - output_1_loss: 0.5552 - output_2_loss: 1.2392 - val_loss: 0.5259 - val_output_1_loss: 0.4557 - val_output_2_loss: 1.1577\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 764us/step - loss: 0.5422 - output_1_loss: 0.4821 - output_2_loss: 1.0825 - val_loss: 0.4889 - val_output_1_loss: 0.4301 - val_output_2_loss: 1.0185\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 736us/step - loss: 0.5382 - output_1_loss: 0.4892 - output_2_loss: 0.9796 - val_loss: 0.4808 - val_output_1_loss: 0.4307 - val_output_2_loss: 0.9317\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.4940 - output_1_loss: 0.4526 - output_2_loss: 0.8671 - val_loss: 0.4573 - val_output_1_loss: 0.4177 - val_output_2_loss: 0.8137\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.4713 - output_1_loss: 0.4366 - output_2_loss: 0.7840 - val_loss: 0.4390 - val_output_1_loss: 0.4050 - val_output_2_loss: 0.7455\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 742us/step - loss: 0.4600 - output_1_loss: 0.4312 - output_2_loss: 0.7192 - val_loss: 0.4375 - val_output_1_loss: 0.4084 - val_output_2_loss: 0.6992\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.4604 - output_1_loss: 0.4355 - output_2_loss: 0.6851 - val_loss: 0.4207 - val_output_1_loss: 0.3942 - val_output_2_loss: 0.6594\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.4340 - output_1_loss: 0.4115 - output_2_loss: 0.6362 - val_loss: 0.4024 - val_output_1_loss: 0.3798 - val_output_2_loss: 0.6059\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 729us/step - loss: 0.6712 - output_1_loss: 0.6562 - output_2_loss: 0.8069 - val_loss: 0.4744 - val_output_1_loss: 0.4414 - val_output_2_loss: 0.7716\n",
      "162/162 [==============================] - 0s 491us/step - loss: 0.4992 - output_1_loss: 0.4693 - output_2_loss: 0.7677\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001ABA286A950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Model: \"wide_and_deep_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            multiple                  210       \n",
      "                                                                 \n",
      " dense_14 (Dense)            multiple                  930       \n",
      "                                                                 \n",
      " dense_15 (Dense)            multiple                  36        \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,207\n",
      "Trainable params: 1,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# veamos un resumen del modelo\n",
    "\n",
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    (X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardando y restaurando un modelo\n",
    "\n",
    "Una vez que hemos entrenado un modelo, es muy útil poder guardar sus parámetros y luego restaurarlos cuando queramos. Esto nos permite continuar entrenando el modelo, hacer predicciones, o simplemente usar el modelo sin tener que volver a entrenarlo. Para guardar un modelo, podemos usar el método save() de Keras. Por ejemplo, para guardar el modelo anterior, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 993us/step - loss: 1.7114 - accuracy: 0.0026 - val_loss: 1.3850 - val_accuracy: 0.0047\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 819us/step - loss: 1.2764 - accuracy: 0.0027 - val_loss: 1.1991 - val_accuracy: 0.0039\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 810us/step - loss: 1.1578 - accuracy: 0.0023 - val_loss: 1.1231 - val_accuracy: 0.0036\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 795us/step - loss: 1.1012 - accuracy: 0.0022 - val_loss: 1.0806 - val_accuracy: 0.0036\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 793us/step - loss: 1.0659 - accuracy: 0.0022 - val_loss: 1.0530 - val_accuracy: 0.0036\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 805us/step - loss: 1.0415 - accuracy: 0.0022 - val_loss: 1.0333 - val_accuracy: 0.0034\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 789us/step - loss: 1.0232 - accuracy: 0.0025 - val_loss: 1.0220 - val_accuracy: 0.0036\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 799us/step - loss: 1.0076 - accuracy: 0.0025 - val_loss: 1.0098 - val_accuracy: 0.0036\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.9978 - accuracy: 0.0025 - val_loss: 0.9959 - val_accuracy: 0.0039\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.9877 - accuracy: 0.0026 - val_loss: 0.9945 - val_accuracy: 0.0039\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.9803 - accuracy: 0.0025 - val_loss: 0.9853 - val_accuracy: 0.0039\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.9713 - accuracy: 0.0026 - val_loss: 0.9797 - val_accuracy: 0.0039\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.9658 - accuracy: 0.0023 - val_loss: 0.9799 - val_accuracy: 0.0039\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.9593 - accuracy: 0.0025 - val_loss: 0.9712 - val_accuracy: 0.0039\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.9545 - accuracy: 0.0025 - val_loss: 0.9658 - val_accuracy: 0.0039\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.9494 - accuracy: 0.0025 - val_loss: 0.9651 - val_accuracy: 0.0039\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.9427 - accuracy: 0.0025 - val_loss: 0.9622 - val_accuracy: 0.0039\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.9373 - accuracy: 0.0025 - val_loss: 0.9565 - val_accuracy: 0.0039\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.9328 - accuracy: 0.0024 - val_loss: 0.9547 - val_accuracy: 0.0039\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.9283 - accuracy: 0.0026 - val_loss: 0.9524 - val_accuracy: 0.0039\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 796us/step - loss: 0.9237 - accuracy: 0.0026 - val_loss: 0.9506 - val_accuracy: 0.0036\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.9197 - accuracy: 0.0027 - val_loss: 0.9468 - val_accuracy: 0.0036\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.9147 - accuracy: 0.0028 - val_loss: 0.9423 - val_accuracy: 0.0039\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 787us/step - loss: 0.9104 - accuracy: 0.0025 - val_loss: 0.9381 - val_accuracy: 0.0039\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 775us/step - loss: 0.9074 - accuracy: 0.0025 - val_loss: 0.9372 - val_accuracy: 0.0036\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.9020 - accuracy: 0.0027 - val_loss: 0.9376 - val_accuracy: 0.0036\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.8987 - accuracy: 0.0024 - val_loss: 0.9319 - val_accuracy: 0.0039\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.8961 - accuracy: 0.0023 - val_loss: 0.9332 - val_accuracy: 0.0039\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.8926 - accuracy: 0.0024 - val_loss: 0.9322 - val_accuracy: 0.0036\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.8882 - accuracy: 0.0026 - val_loss: 0.9300 - val_accuracy: 0.0039\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[8]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30,\n",
    "    validation_data=(X_valid, y_valid))\n",
    "\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras utiliza el formato Protocol Buffers para guardar los modelos. Este formato es muy eficiente, y es el formato utilizado por TensorFlow para guardar los modelos. Por defecto, Keras guarda los modelos en el formato HDF5, que es un formato muy popular para guardar datos estructurados. Este formato es compatible con muchos lenguajes de programación, y es fácil de usar desde Python. Si queremos guardar el modelo en otro formato, podemos hacerlo especificando el parámetro save_format. \n",
    "\n",
    "Para recuperar un modelo guardado, podemos usar el método load_model() de Keras. Por ejemplo, para recuperar el modelo anterior, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks para guardar y restaurar modelos\n",
    "\n",
    "¿Pero qué pasa si el entrenamiento dura mucho tiempo, y queremos guardar el modelo cada cierto tiempo, para poder restaurarlo en caso de fallo?\n",
    "\n",
    "En este caso deberíamos guardar el modelo cada cierto tiempo, y luego restaurarlo si falla. Para hacer esto, podemos usar callbacks. Un callback es un objeto que se pasa al método fit() de Keras, y que Keras llama en ciertos momentos durante el entrenamiento (por ejemplo, al comienzo y al final de cada época). Por ejemplo, podemos crear un callback que guarde el modelo a intervalos regulares, y luego restaurarlo si falla. Para hacer esto, podemos crear una clase que herede de la clase Callback de Keras, y definir los métodos on_epoch_end() y on_train_begin(). El método on_epoch_end() se llama al final de cada época, y el método on_train_begin() se llama al comienzo del entrenamiento. Por ejemplo, para guardar el modelo cada a intervalos regulares, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.8858 - accuracy: 0.0023\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.8819 - accuracy: 0.0025\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 629us/step - loss: 0.8796 - accuracy: 0.0028\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.8758 - accuracy: 0.0025\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.8728 - accuracy: 0.0028\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.8711 - accuracy: 0.0026\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 631us/step - loss: 0.8675 - accuracy: 0.0028\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.8650 - accuracy: 0.0025\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.8627 - accuracy: 0.0026\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.8596 - accuracy: 0.0026\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es más, si usamos un conjunto de validación, podemos guardar el modelo que obtenga el mejor rendimiento en el conjunto de validación. Podemos asignar el parámetro save_best_only = True durante la creación del objeto ModelCheckpoint, y Keras solo guardará el modelo si mejora el rendimiento en el conjunto de validación.\n",
    "\n",
    "De esta manera no tienes que preocuparte por entrenar durante mucho tiempo y sobreajustar el modelo, ya que Keras solo guardará el modelo que obtenga el mejor rendimiento en el conjunto de validación. Este código es una manera sencilla de implementar el algoritmo de early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8579 - accuracy: 0.0027 - val_loss: 0.9132 - val_accuracy: 0.0039\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.8554 - accuracy: 0.0027 - val_loss: 0.9145 - val_accuracy: 0.0041\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.8534 - accuracy: 0.0029 - val_loss: 0.9089 - val_accuracy: 0.0039\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.8508 - accuracy: 0.0027 - val_loss: 0.9102 - val_accuracy: 0.0039\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.8503 - accuracy: 0.0029 - val_loss: 0.9125 - val_accuracy: 0.0039\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.8473 - accuracy: 0.0027 - val_loss: 0.9057 - val_accuracy: 0.0041\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.8440 - accuracy: 0.0029 - val_loss: 0.9063 - val_accuracy: 0.0041\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 852us/step - loss: 0.8423 - accuracy: 0.0027 - val_loss: 0.9026 - val_accuracy: 0.0036\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 798us/step - loss: 0.8412 - accuracy: 0.0030 - val_loss: 0.9029 - val_accuracy: 0.0039\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 814us/step - loss: 0.8386 - accuracy: 0.0028 - val_loss: 0.9072 - val_accuracy: 0.0041\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    " save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra manera de implementar early stopping es usar el parámetro patience del objeto EarlyStopping. Este parámetro especifica el número de épocas que Keras debe esperar antes de detener el entrenamiento si no mejora el rendimiento en el conjunto de validación. Por ejemplo, podemos crear un objeto EarlyStopping con el parámetro patience = 10, y Keras detendrá el entrenamiento si no mejora el rendimiento en el conjunto de validación durante 10 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8406 - accuracy: 0.0029 - val_loss: 0.9047 - val_accuracy: 0.0041\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.8398 - accuracy: 0.0027 - val_loss: 0.9016 - val_accuracy: 0.0039\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.8368 - accuracy: 0.0026 - val_loss: 0.9009 - val_accuracy: 0.0044\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 802us/step - loss: 0.8358 - accuracy: 0.0028 - val_loss: 0.9061 - val_accuracy: 0.0041\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.8335 - accuracy: 0.0027 - val_loss: 0.9044 - val_accuracy: 0.0041\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 853us/step - loss: 0.8307 - accuracy: 0.0028 - val_loss: 0.9008 - val_accuracy: 0.0047\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.8290 - accuracy: 0.0028 - val_loss: 0.9102 - val_accuracy: 0.0052\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 808us/step - loss: 0.8280 - accuracy: 0.0028 - val_loss: 0.9048 - val_accuracy: 0.0044\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.8274 - accuracy: 0.0028 - val_loss: 0.8947 - val_accuracy: 0.0039\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.8249 - accuracy: 0.0028 - val_loss: 0.8989 - val_accuracy: 0.0041\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.8248 - accuracy: 0.0030 - val_loss: 0.8972 - val_accuracy: 0.0041\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.8222 - accuracy: 0.0028 - val_loss: 0.9012 - val_accuracy: 0.0036\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 806us/step - loss: 0.8197 - accuracy: 0.0028 - val_loss: 0.9030 - val_accuracy: 0.0047\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 850us/step - loss: 0.8184 - accuracy: 0.0030 - val_loss: 0.8925 - val_accuracy: 0.0047\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.8176 - accuracy: 0.0027 - val_loss: 0.8935 - val_accuracy: 0.0044\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.8183 - accuracy: 0.0030 - val_loss: 0.8966 - val_accuracy: 0.0039\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.8151 - accuracy: 0.0030 - val_loss: 0.8906 - val_accuracy: 0.0041\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.8146 - accuracy: 0.0028 - val_loss: 0.8934 - val_accuracy: 0.0041\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.8124 - accuracy: 0.0029 - val_loss: 0.9000 - val_accuracy: 0.0041\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.8106 - accuracy: 0.0030 - val_loss: 0.8904 - val_accuracy: 0.0039\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.8103 - accuracy: 0.0029 - val_loss: 0.9010 - val_accuracy: 0.0044\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.8090 - accuracy: 0.0027 - val_loss: 0.8909 - val_accuracy: 0.0047\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.8074 - accuracy: 0.0030 - val_loss: 0.8919 - val_accuracy: 0.0047\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.8072 - accuracy: 0.0028 - val_loss: 0.8892 - val_accuracy: 0.0041\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 851us/step - loss: 0.8055 - accuracy: 0.0028 - val_loss: 0.8878 - val_accuracy: 0.0047\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 864us/step - loss: 0.8038 - accuracy: 0.0031 - val_loss: 0.8868 - val_accuracy: 0.0044\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.8039 - accuracy: 0.0028 - val_loss: 0.8858 - val_accuracy: 0.0044\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.8017 - accuracy: 0.0028 - val_loss: 0.8960 - val_accuracy: 0.0039\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 851us/step - loss: 0.8018 - accuracy: 0.0026 - val_loss: 0.8865 - val_accuracy: 0.0047\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.8009 - accuracy: 0.0029 - val_loss: 0.8846 - val_accuracy: 0.0049\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 852us/step - loss: 0.7982 - accuracy: 0.0028 - val_loss: 0.8858 - val_accuracy: 0.0041\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.7981 - accuracy: 0.0029 - val_loss: 0.8863 - val_accuracy: 0.0047\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.7988 - accuracy: 0.0027 - val_loss: 0.8862 - val_accuracy: 0.0041\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 783us/step - loss: 0.7955 - accuracy: 0.0032 - val_loss: 0.8911 - val_accuracy: 0.0041\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.7949 - accuracy: 0.0028 - val_loss: 0.8903 - val_accuracy: 0.0047\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 819us/step - loss: 0.7956 - accuracy: 0.0027 - val_loss: 0.8845 - val_accuracy: 0.0047\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 824us/step - loss: 0.7932 - accuracy: 0.0027 - val_loss: 0.8792 - val_accuracy: 0.0047\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.7927 - accuracy: 0.0030 - val_loss: 0.8833 - val_accuracy: 0.0044\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.7919 - accuracy: 0.0029 - val_loss: 0.8882 - val_accuracy: 0.0047\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.7905 - accuracy: 0.0029 - val_loss: 0.8821 - val_accuracy: 0.0044\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.7892 - accuracy: 0.0028 - val_loss: 0.8861 - val_accuracy: 0.0039\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 805us/step - loss: 0.7879 - accuracy: 0.0028 - val_loss: 0.8800 - val_accuracy: 0.0047\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.7873 - accuracy: 0.0029 - val_loss: 0.8841 - val_accuracy: 0.0047\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 806us/step - loss: 0.7875 - accuracy: 0.0031 - val_loss: 0.8794 - val_accuracy: 0.0041\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.7877 - accuracy: 0.0028 - val_loss: 0.8806 - val_accuracy: 0.0039\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.7859 - accuracy: 0.0028 - val_loss: 0.8764 - val_accuracy: 0.0049\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.7842 - accuracy: 0.0027 - val_loss: 0.8789 - val_accuracy: 0.0047\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 846us/step - loss: 0.7839 - accuracy: 0.0028 - val_loss: 0.8752 - val_accuracy: 0.0044\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.7824 - accuracy: 0.0028 - val_loss: 0.8800 - val_accuracy: 0.0047\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.7828 - accuracy: 0.0030 - val_loss: 0.8855 - val_accuracy: 0.0044\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.7814 - accuracy: 0.0028 - val_loss: 0.8746 - val_accuracy: 0.0044\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.7800 - accuracy: 0.0028 - val_loss: 0.8895 - val_accuracy: 0.0041\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 799us/step - loss: 0.7806 - accuracy: 0.0029 - val_loss: 0.8774 - val_accuracy: 0.0047\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.7793 - accuracy: 0.0029 - val_loss: 0.8731 - val_accuracy: 0.0044\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.7779 - accuracy: 0.0028 - val_loss: 0.8779 - val_accuracy: 0.0047\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.7775 - accuracy: 0.0029 - val_loss: 0.8783 - val_accuracy: 0.0049\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.7762 - accuracy: 0.0031 - val_loss: 0.8750 - val_accuracy: 0.0049\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.7768 - accuracy: 0.0032 - val_loss: 0.8828 - val_accuracy: 0.0044\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.7754 - accuracy: 0.0029 - val_loss: 0.8751 - val_accuracy: 0.0047\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.7757 - accuracy: 0.0028 - val_loss: 0.8761 - val_accuracy: 0.0039\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 799us/step - loss: 0.7724 - accuracy: 0.0029 - val_loss: 0.8887 - val_accuracy: 0.0047\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 841us/step - loss: 0.7745 - accuracy: 0.0030 - val_loss: 0.8704 - val_accuracy: 0.0049\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.7722 - accuracy: 0.0028 - val_loss: 0.8811 - val_accuracy: 0.0044\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.7728 - accuracy: 0.0028 - val_loss: 0.8821 - val_accuracy: 0.0047\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.7712 - accuracy: 0.0028 - val_loss: 0.8757 - val_accuracy: 0.0039\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.7710 - accuracy: 0.0030 - val_loss: 0.8722 - val_accuracy: 0.0047\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.7690 - accuracy: 0.0029 - val_loss: 0.8758 - val_accuracy: 0.0049\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.7694 - accuracy: 0.0031 - val_loss: 0.8766 - val_accuracy: 0.0047\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.7697 - accuracy: 0.0033 - val_loss: 0.8733 - val_accuracy: 0.0044\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.7689 - accuracy: 0.0027 - val_loss: 0.8932 - val_accuracy: 0.0044\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.7672 - accuracy: 0.0031 - val_loss: 0.8714 - val_accuracy: 0.0041\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.7670 - accuracy: 0.0029 - val_loss: 0.8772 - val_accuracy: 0.0039\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    " restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de épocas que Keras debe esperar antes de detener el entrenamiento se llama el número de épocas de paciencia. Si el rendimiento en el conjunto de validación no mejora durante el número de épocas de paciencia, Keras detendrá el entrenamiento. Si el rendimiento en el conjunto de validación mejora, Keras reiniciará el número de épocas de paciencia, y esperará otros 10 épocas antes de detener el entrenamiento. Si el rendimiento en el conjunto de validación no mejora durante 10 épocas, Keras detendrá el entrenamiento. Si el rendimiento en el conjunto de validación mejora, Keras reiniciará el número de épocas de paciencia, y esperará otros 10 épocas antes de detener el entrenamiento. Y así sucesivamente.\n",
    "\n",
    "Si necesitas de un mayor grado de control sobre el entrenamiento, puedes crear tus propios callbacks. Para hacer esto, debes crear una clase que herede de la clase Callback de Keras, y definir los métodos que quieras. Por ejemplo, podemos crear un callback que muestre el ratio entre el error de entrenamiento y el error de validación al final de cada época. Para hacer esto, podemos crear una clase que herede de la clase Callback de Keras, y definir el método on_epoch_end(). El método on_epoch_end() se llama al final de cada época, y recibe como parámetro los logs, que es un diccionario que contiene el error de entrenamiento y el error de validación. Por ejemplo, podemos crear un callback que muestre el ratio entre el error de entrenamiento y el error de validación al final de cada época de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    " def on_epoch_end(self, epoch, logs):\n",
    "    print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos implementar los callbacks en on_train_begin() y on_train_end() para realizar acciones al comienzo y al final del entrenamiento. En on_epoch_begin() y on_epoch_end() podemos realizar acciones al comienzo y al final de cada época. En on_batch_begin() y on_batch_end() podemos realizar acciones al comienzo y al final de cada lote. En on_test_begin() y on_test_end() podemos realizar acciones al comienzo y al final de la evaluación. En on_predict_begin() y on_predict_end() podemos realizar acciones al comienzo y al final de las predicciones. En on_train_batch_begin() y on_train_batch_end() podemos realizar acciones al comienzo y al final de cada lote de entrenamiento. En on_test_batch_begin() y on_test_batch_end() podemos realizar acciones al comienzo y al final de cada lote de evaluación. En on_predict_batch_begin() y on_predict_batch_end() podemos realizar acciones al comienzo y al final de cada lote de predicción.\n",
    "\n",
    "## Usando TensorBoard para visualizar el entrenamiento\n",
    "\n",
    "Finalmente vamos a conocer una herramienta muy útil para el uso de tf.keras: TensorBoard. TensorBoard es una herramienta de visualización de TensorFlow que nos permite visualizar los resultados de nuestro modelo. Por ejemplo, podemos visualizar el error de entrenamiento y el error de validación en cada época, y podemos visualizar el rendimiento de nuestro modelo en el conjunto de validación. Para usar TensorBoard, debemos crear un objeto TensorBoard, y pasar el objeto a la función fit() del modelo. Por ejemplo, podemos crear un objeto TensorBoard con el parámetro log_dir = 'logs', y pasar el objeto a la función fit() del modelo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "def get_run_logdir():\n",
    " import time\n",
    " run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    " return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 0s 891us/step - loss: 0.7728 - accuracy: 0.0030 - val_loss: 0.8733 - val_accuracy: 0.0044\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 838us/step - loss: 0.7714 - accuracy: 0.0031 - val_loss: 0.8764 - val_accuracy: 0.0044\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.7731 - accuracy: 0.0032 - val_loss: 0.8689 - val_accuracy: 0.0044\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.7702 - accuracy: 0.0029 - val_loss: 0.8758 - val_accuracy: 0.0044\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.7702 - accuracy: 0.0030 - val_loss: 0.8799 - val_accuracy: 0.0041\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.7710 - accuracy: 0.0028 - val_loss: 0.8773 - val_accuracy: 0.0044\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.7697 - accuracy: 0.0028 - val_loss: 0.8719 - val_accuracy: 0.0049\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.7681 - accuracy: 0.0031 - val_loss: 0.8761 - val_accuracy: 0.0044\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.7678 - accuracy: 0.0028 - val_loss: 0.8756 - val_accuracy: 0.0044\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.7662 - accuracy: 0.0032 - val_loss: 0.8834 - val_accuracy: 0.0039\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 814us/step - loss: 0.7663 - accuracy: 0.0033 - val_loss: 0.8722 - val_accuracy: 0.0041\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 824us/step - loss: 0.7662 - accuracy: 0.0027 - val_loss: 0.8716 - val_accuracy: 0.0044\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 829us/step - loss: 0.7642 - accuracy: 0.0030 - val_loss: 0.8864 - val_accuracy: 0.0044\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 846us/step - loss: 0.7643 - accuracy: 0.0029 - val_loss: 0.8720 - val_accuracy: 0.0047\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.7639 - accuracy: 0.0029 - val_loss: 0.8712 - val_accuracy: 0.0041\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 816us/step - loss: 0.7633 - accuracy: 0.0029 - val_loss: 0.8779 - val_accuracy: 0.0044\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.7620 - accuracy: 0.0033 - val_loss: 0.8772 - val_accuracy: 0.0041\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.7617 - accuracy: 0.0032 - val_loss: 0.8780 - val_accuracy: 0.0047\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 816us/step - loss: 0.7634 - accuracy: 0.0031 - val_loss: 0.8695 - val_accuracy: 0.0041\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 850us/step - loss: 0.7624 - accuracy: 0.0030 - val_loss: 0.8659 - val_accuracy: 0.0041\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 879us/step - loss: 0.7591 - accuracy: 0.0031 - val_loss: 0.8726 - val_accuracy: 0.0039\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.7602 - accuracy: 0.0030 - val_loss: 0.8760 - val_accuracy: 0.0036\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.7584 - accuracy: 0.0028 - val_loss: 0.8734 - val_accuracy: 0.0041\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 871us/step - loss: 0.7598 - accuracy: 0.0031 - val_loss: 0.8734 - val_accuracy: 0.0049\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 877us/step - loss: 0.7575 - accuracy: 0.0030 - val_loss: 0.8720 - val_accuracy: 0.0041\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 876us/step - loss: 0.7582 - accuracy: 0.0029 - val_loss: 0.8722 - val_accuracy: 0.0039\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 878us/step - loss: 0.7551 - accuracy: 0.0030 - val_loss: 0.8709 - val_accuracy: 0.0036\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.7567 - accuracy: 0.0029 - val_loss: 0.8835 - val_accuracy: 0.0039\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.7553 - accuracy: 0.0031 - val_loss: 0.8793 - val_accuracy: 0.0044\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 902us/step - loss: 0.7562 - accuracy: 0.0031 - val_loss: 0.8769 - val_accuracy: 0.0036\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[tensorboard_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería de TensorBoard ofrece una API de bajo nivel con el paquete tf.summary, el siguiente código utiliza la función create_file_writer() para crear un SummaryWritter, y lo utiliza para escribir un registro escalares, histogramas, imágenes, audio y texto, que pueden ser visualizados usando TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    " for step in range(1, 1000 + 1):\n",
    "    tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "    data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "    tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "    images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\n",
    "    tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "    texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "    tf.summary.text(\"my_text\", texts, step=step)\n",
    "    sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "    audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "    tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 22668), started 0:00:15 ago. (Use '!kill 22668' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-67185dfba86082b4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-67185dfba86082b4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de hiperparámetros de la red neuronal\n",
    "\n",
    "La flexibilidad de las redes neuronales es también uno de sus inconvenientes: hay que afinar muchos parámetros para obtener un buen rendimiento. Tenemos que elegir el número de capas, el número de neuronas por capa, la función de activación de cada capa, el número de épocas de entrenamiento, el tamaño del lote, el optimizador, la tasa de aprendizaje, el momento, etc. Hay muchos parámetros que podemos ajustar, y es difícil saber cuáles son los mejores. \n",
    "\n",
    "Una opción es usar GridSearchCV o RandomizedSearchCV para explorar el espacio de hiperparámetros. Por ejemplo, podemos crear un objeto GridSearchCV, y pasarle el modelo, el espacio de hiperparámetros, el número de validaciones cruzadas, y el número de procesadores que queremos usar.\n",
    "\n",
    "Para hacer esto, primero tendremos que envolver nuestros modelos de Keras en un objeto que imite a los regresores de Scikit-Learn. Para hacer esto, podemos crear una clase que herede de la clase BaseEstimator de Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función crea un modelo Secuencial simple para una regresión univariante, con una capa de entrada, una serie de capas ocultas y una capa de salida, se compila usando un optimizador SGD configurado con el parámetro learning_rate.\n",
    "\n",
    "Vamos a crear un KerasRegressor basado en la función build_model():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdriánPortilloSánche\\AppData\\Local\\Temp\\ipykernel_10532\\1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este objeto se puede usar como cualquier otro regresor de Scikit-Learn. Por ejemplo, podemos entrenarlo usando el método fit(), evaluarlo usando el método score(), y hacer predicciones usando el método predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 45s - loss: 4.8188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdriánPortilloSánche\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 792us/step - loss: 1.4475 - val_loss: 0.7006\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 634us/step - loss: 0.7053 - val_loss: 0.6231\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.6430 - val_loss: 0.5767\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.5964 - val_loss: 0.5350\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.5766 - val_loss: 0.5367\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 636us/step - loss: 0.5610 - val_loss: 0.5624\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.6520 - val_loss: 0.6280\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 629us/step - loss: 0.6696 - val_loss: 0.4833\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.5126 - val_loss: 0.4535\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.4906 - val_loss: 0.4561\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 651us/step - loss: 0.4820 - val_loss: 0.4401\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 647us/step - loss: 0.4714 - val_loss: 0.4304\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.4683 - val_loss: 0.4266\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.4613 - val_loss: 0.4224\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.4572 - val_loss: 0.4203\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.4513 - val_loss: 0.4144\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.4489 - val_loss: 0.4118\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.4435 - val_loss: 0.4134\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.4397 - val_loss: 0.4066\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 637us/step - loss: 0.4368 - val_loss: 0.4066\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.4332 - val_loss: 0.4035\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.4301 - val_loss: 0.3987\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.4272 - val_loss: 0.3986\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.4242 - val_loss: 0.3968\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 604us/step - loss: 0.4215 - val_loss: 0.3934\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.4182 - val_loss: 0.3923\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.4155 - val_loss: 0.3893\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.4148 - val_loss: 0.3914\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.4118 - val_loss: 0.3880\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4098 - val_loss: 0.3865\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.4068 - val_loss: 0.3886\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.4050 - val_loss: 0.3833\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.4025 - val_loss: 0.3810\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.4017 - val_loss: 0.3827\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.3991 - val_loss: 0.3785\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.3973 - val_loss: 0.3801\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 637us/step - loss: 0.3959 - val_loss: 0.3761\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 643us/step - loss: 0.3938 - val_loss: 0.3752\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.3927 - val_loss: 0.3740\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.3915 - val_loss: 0.3744\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.3920 - val_loss: 0.3727\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3886 - val_loss: 0.3734\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3867 - val_loss: 0.3702\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.3878 - val_loss: 0.3718\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.3864 - val_loss: 0.3693\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3836 - val_loss: 0.3677\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 641us/step - loss: 0.3821 - val_loss: 0.3677\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 623us/step - loss: 0.3815 - val_loss: 0.3727\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 643us/step - loss: 0.3822 - val_loss: 0.3661\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 611us/step - loss: 0.3793 - val_loss: 0.3645\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 613us/step - loss: 0.3780 - val_loss: 0.3628\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.3772 - val_loss: 0.3648\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3770 - val_loss: 0.3634\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 604us/step - loss: 0.3752 - val_loss: 0.3769\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.3753 - val_loss: 0.3611\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.3734 - val_loss: 0.3604\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.3729 - val_loss: 0.3623\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3732 - val_loss: 0.3587\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.3716 - val_loss: 0.3586\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3707 - val_loss: 0.3602\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.3698 - val_loss: 0.3579\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.3754 - val_loss: 0.3572\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.3698 - val_loss: 0.3596\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.3680 - val_loss: 0.3577\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.3679 - val_loss: 0.3594\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3691 - val_loss: 0.3559\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 611us/step - loss: 0.3673 - val_loss: 0.3551\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.3650 - val_loss: 0.3570\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3647 - val_loss: 0.3542\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.3637 - val_loss: 0.3563\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.3638 - val_loss: 0.3545\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 603us/step - loss: 0.3647 - val_loss: 0.3516\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.3623 - val_loss: 0.3519\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.3614 - val_loss: 0.3564\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3616 - val_loss: 0.3511\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.3609 - val_loss: 0.3512\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3609 - val_loss: 0.3540\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 654us/step - loss: 0.3590 - val_loss: 0.3504\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 610us/step - loss: 0.3607 - val_loss: 0.3510\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.3609 - val_loss: 0.3519\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.3592 - val_loss: 0.3497\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.3575 - val_loss: 0.3505\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.3571 - val_loss: 0.3487\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3563 - val_loss: 0.3473\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 618us/step - loss: 0.3569 - val_loss: 0.3463\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.3571 - val_loss: 0.3472\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.3597 - val_loss: 0.3481\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.3553 - val_loss: 0.3462\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 630us/step - loss: 0.3543 - val_loss: 0.3456\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.3539 - val_loss: 0.3451\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3535 - val_loss: 0.3449\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.3534 - val_loss: 0.3442\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.3598 - val_loss: 0.3473\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.3540 - val_loss: 0.3476\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 621us/step - loss: 0.3568 - val_loss: 0.3437\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.3522 - val_loss: 0.3420\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.3523 - val_loss: 0.3423\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 635us/step - loss: 0.3512 - val_loss: 0.3439\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 637us/step - loss: 0.3566 - val_loss: 0.3513\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.3506 - val_loss: 0.3507\n",
      "162/162 [==============================] - 0s 441us/step - loss: 0.3684\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001AB9EBCAC20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cualquier parámetro que se pase al método fit() se pasarán al método fit() del modelo de Keras.\n",
    "\n",
    "Además, la puntuación será la contraria del error cuadrático medio (MSE), ya que Scikit-Learn maximiza las puntuaciones, mientras que Keras minimiza las pérdidas.\n",
    "\n",
    "Pero no queremos simplemente entrenar un modelo, queremos entrenar sus cientos de variantes para ver cual obtiene mejor rendimiento en el conjunto de validación. Ya que hay muchos parámetros, será mejor usar random search en lugar de grid search. Para ello, podemos crear un objeto RandomizedSearchCV, vamos a explorar el número de capas ocultas, el número de neuronas por capa y la tasa de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 1.7294 - val_loss: 22.0516\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 433us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.9050 - val_loss: 0.6405\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.6560 - val_loss: 0.5015\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5291 - val_loss: 0.4650\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4910 - val_loss: 0.4466\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4675 - val_loss: 0.4656\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4533 - val_loss: 0.4584\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4409 - val_loss: 0.4964\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.4359 - val_loss: 0.5072\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4252 - val_loss: 0.5340\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.4205 - val_loss: 0.5388\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4148 - val_loss: 0.5552\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4075 - val_loss: 0.5792\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.4043 - val_loss: 0.5930\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4009 - val_loss: 0.5987\n",
      "121/121 [==============================] - 0s 458us/step - loss: 1.2750\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 1.3410 - val_loss: 10.4006\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 442us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 1.4511 - val_loss: 0.6699\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.6923 - val_loss: 0.5862\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.6142 - val_loss: 0.5153\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.5532 - val_loss: 0.4818\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.5008 - val_loss: 0.4634\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4811 - val_loss: 0.4397\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.4666 - val_loss: 0.4317\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.4557 - val_loss: 0.4181\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4465 - val_loss: 0.4152\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.4397 - val_loss: 0.4058\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.4316 - val_loss: 0.4004\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.4253 - val_loss: 0.3970\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4199 - val_loss: 0.3928\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4145 - val_loss: 0.3883\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4094 - val_loss: 0.3864\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4067 - val_loss: 0.3807\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4016 - val_loss: 0.3792\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3982 - val_loss: 0.3801\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3944 - val_loss: 0.3751\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3910 - val_loss: 0.3715\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3881 - val_loss: 0.3700\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3849 - val_loss: 0.3705\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3818 - val_loss: 0.3652\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3789 - val_loss: 0.3644\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3765 - val_loss: 0.3666\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3731 - val_loss: 0.3612\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3705 - val_loss: 0.3658\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3681 - val_loss: 0.3577\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3660 - val_loss: 0.3572\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3642 - val_loss: 0.3549\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3621 - val_loss: 0.3584\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3613 - val_loss: 0.3569\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3627 - val_loss: 0.3561\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3666 - val_loss: 0.3563\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3645 - val_loss: 0.3573\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3603 - val_loss: 0.3482\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3535 - val_loss: 0.3477\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3496 - val_loss: 0.3476\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3464 - val_loss: 0.3453\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3445 - val_loss: 0.3431\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3423 - val_loss: 0.3439\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3409 - val_loss: 0.3431\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3391 - val_loss: 0.3397\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3375 - val_loss: 0.3429\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3361 - val_loss: 0.3408\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.3350 - val_loss: 0.3371\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3333 - val_loss: 0.3464\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3319 - val_loss: 0.3379\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3307 - val_loss: 0.3381\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3294 - val_loss: 0.3338\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3281 - val_loss: 0.3331\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3270 - val_loss: 0.3312\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3260 - val_loss: 0.3313\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3241 - val_loss: 0.3355\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3239 - val_loss: 0.3308\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3238 - val_loss: 0.3287\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3219 - val_loss: 0.3315\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3216 - val_loss: 0.3292\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3233 - val_loss: 0.3400\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3261 - val_loss: 0.3343\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.3345 - val_loss: 0.3420\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.3454 - val_loss: 0.3345\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3356 - val_loss: 0.3260\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3169 - val_loss: 0.3231\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3143 - val_loss: 0.3236\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3140 - val_loss: 0.3234\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3132 - val_loss: 0.3215\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3122 - val_loss: 0.3267\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3101 - val_loss: 0.3240\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3092 - val_loss: 0.3230\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3084 - val_loss: 0.3192\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3081 - val_loss: 0.3240\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3071 - val_loss: 0.3175\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3063 - val_loss: 0.3204\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3047 - val_loss: 0.3173\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3043 - val_loss: 0.3262\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3034 - val_loss: 0.3190\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3025 - val_loss: 0.3159\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3014 - val_loss: 0.3149\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3009 - val_loss: 0.3137\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3005 - val_loss: 0.3186\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2988 - val_loss: 0.3155\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.2982 - val_loss: 0.3192\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2987 - val_loss: 0.3117\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2977 - val_loss: 0.3131\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2968 - val_loss: 0.3116\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2961 - val_loss: 0.3090\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2948 - val_loss: 0.3143\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2945 - val_loss: 0.3120\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.2943 - val_loss: 0.3114\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2933 - val_loss: 0.3121\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.2920 - val_loss: 0.3119\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2914 - val_loss: 0.3091\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2920 - val_loss: 0.3094\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2922 - val_loss: 0.3066\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.2901 - val_loss: 0.3061\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.2899 - val_loss: 0.3120\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2888 - val_loss: 0.3099\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2892 - val_loss: 0.3047\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2879 - val_loss: 0.3098\n",
      "121/121 [==============================] - 0s 440us/step - loss: 0.3282\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 1.4826 - val_loss: 0.7535\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.7201 - val_loss: 0.6514\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6391 - val_loss: 0.5723\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5806 - val_loss: 0.5175\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5392 - val_loss: 0.4863\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5090 - val_loss: 0.4771\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4922 - val_loss: 0.4546\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4758 - val_loss: 0.4502\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.4673 - val_loss: 0.4453\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4570 - val_loss: 0.4428\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4515 - val_loss: 0.4463\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.4437 - val_loss: 0.4491\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4402 - val_loss: 0.4498\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4344 - val_loss: 0.4501\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4290 - val_loss: 0.4495\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.4246 - val_loss: 0.4515\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4229 - val_loss: 0.4586\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4204 - val_loss: 0.4586\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4214 - val_loss: 0.4586\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.4116 - val_loss: 0.4653\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.6502\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 1.9881 - val_loss: 0.8265\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 1.0842 - val_loss: 1.1335\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 1.5681 - val_loss: 0.6863\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.7105 - val_loss: 0.5366\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5574 - val_loss: 0.4941\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5254 - val_loss: 0.4689\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5018 - val_loss: 0.4521\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4846 - val_loss: 0.4385\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4710 - val_loss: 0.4262\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4586 - val_loss: 0.4193\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.4511 - val_loss: 0.4119\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4436 - val_loss: 0.4053\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4368 - val_loss: 0.4001\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.4313 - val_loss: 0.3972\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4264 - val_loss: 0.3927\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4218 - val_loss: 0.3900\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4166 - val_loss: 0.3887\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4123 - val_loss: 0.3839\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4084 - val_loss: 0.3823\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4054 - val_loss: 0.3805\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4020 - val_loss: 0.3785\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3987 - val_loss: 0.3779\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3960 - val_loss: 0.3736\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3931 - val_loss: 0.3727\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3908 - val_loss: 0.3704\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3878 - val_loss: 0.3681\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3845 - val_loss: 0.3666\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3824 - val_loss: 0.3651\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3798 - val_loss: 0.3656\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3785 - val_loss: 0.3630\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3760 - val_loss: 0.3622\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3741 - val_loss: 0.3608\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3716 - val_loss: 0.3604\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3701 - val_loss: 0.3603\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3685 - val_loss: 0.3564\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3661 - val_loss: 0.3572\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3648 - val_loss: 0.3552\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3631 - val_loss: 0.3528\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3613 - val_loss: 0.3550\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3600 - val_loss: 0.3514\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3573 - val_loss: 0.3514\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3565 - val_loss: 0.3537\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3547 - val_loss: 0.3503\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3533 - val_loss: 0.3493\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3520 - val_loss: 0.3502\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3512 - val_loss: 0.3461\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3493 - val_loss: 0.3459\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3473 - val_loss: 0.3448\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3469 - val_loss: 0.3440\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3455 - val_loss: 0.3434\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.3442 - val_loss: 0.3449\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3433 - val_loss: 0.3421\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.3422 - val_loss: 0.3413\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3408 - val_loss: 0.3403\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3398 - val_loss: 0.3400\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3382 - val_loss: 0.3438\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3379 - val_loss: 0.3402\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3360 - val_loss: 0.3382\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3345 - val_loss: 0.3407\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3345 - val_loss: 0.3365\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3329 - val_loss: 0.3386\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3324 - val_loss: 0.3353\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.3313 - val_loss: 0.3342\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3304 - val_loss: 0.3350\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3289 - val_loss: 0.3344\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.3283 - val_loss: 0.3342\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3279 - val_loss: 0.3336\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3279 - val_loss: 0.3329\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3267 - val_loss: 0.3323\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3255 - val_loss: 0.3321\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3239 - val_loss: 0.3331\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3232 - val_loss: 0.3371\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3227 - val_loss: 0.3298\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3212 - val_loss: 0.3298\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3210 - val_loss: 0.3304\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.3197 - val_loss: 0.3293\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3191 - val_loss: 0.3297\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3178 - val_loss: 0.3276\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3180 - val_loss: 0.3294\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3165 - val_loss: 0.3284\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3159 - val_loss: 0.3256\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3158 - val_loss: 0.3292\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3147 - val_loss: 0.3241\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3144 - val_loss: 0.3239\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3131 - val_loss: 0.3255\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3124 - val_loss: 0.3256\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3107 - val_loss: 0.3276\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.3117 - val_loss: 0.3231\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3099 - val_loss: 0.3211\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3098 - val_loss: 0.3223\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3089 - val_loss: 0.3213\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3084 - val_loss: 0.3213\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3071 - val_loss: 0.3245\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3079 - val_loss: 0.3214\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3064 - val_loss: 0.3200\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3075 - val_loss: 0.3283\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3074 - val_loss: 0.3207\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3092 - val_loss: 0.3247\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3105 - val_loss: 0.3235\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3117 - val_loss: 0.3245\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.3183\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.7772 - val_loss: 1.6904\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 467us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.7082 - val_loss: 0.5081\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4628 - val_loss: 0.4716\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4230 - val_loss: 0.4299\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4092 - val_loss: 0.4124\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4012 - val_loss: 0.3879\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3866 - val_loss: 0.3776\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3923 - val_loss: 0.3825\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.3751 - val_loss: 0.3591\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3639 - val_loss: 0.3592\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3581 - val_loss: 0.3885\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3519 - val_loss: 0.3489\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3488 - val_loss: 0.3710\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3585 - val_loss: 0.3503\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3437 - val_loss: 0.3847\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3402 - val_loss: 0.3486\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3330 - val_loss: 0.3567\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3292 - val_loss: 0.3540\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3363 - val_loss: 0.3830\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3291 - val_loss: 0.3139\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3222 - val_loss: 0.3580\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3185 - val_loss: 0.3189\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3197 - val_loss: 0.3236\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3283 - val_loss: 0.3178\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3246 - val_loss: 0.3318\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3173 - val_loss: 0.3931\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3131 - val_loss: 0.3201\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3095 - val_loss: 0.3121\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3128 - val_loss: 0.3155\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3072 - val_loss: 0.3123\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3074 - val_loss: 0.3145\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3060 - val_loss: 0.3060\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2990 - val_loss: 0.3184\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2999 - val_loss: 0.3271\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2982 - val_loss: 0.3150\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2968 - val_loss: 0.3118\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2953 - val_loss: 0.3085\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.2979 - val_loss: 0.3152\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.2963 - val_loss: 0.3123\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2929 - val_loss: 0.3348\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2936 - val_loss: 0.3192\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2904 - val_loss: 0.3169\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.3503\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 1.1909 - val_loss: 1.4145\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 433us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 6.6330 - val_loss: 3.7093\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 2.7754 - val_loss: 2.0303\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 1.7000 - val_loss: 1.4070\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 1.2701 - val_loss: 1.1077\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 1.0467 - val_loss: 0.9403\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.9178 - val_loss: 0.8419\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.8414 - val_loss: 0.7823\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.7940 - val_loss: 0.7433\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.7623 - val_loss: 0.7166\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.7395 - val_loss: 0.6965\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.7229 - val_loss: 0.6813\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.7090 - val_loss: 0.6685\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.6973 - val_loss: 0.6578\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6872 - val_loss: 0.6481\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.6782 - val_loss: 0.6398\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.6701 - val_loss: 0.6319\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6626 - val_loss: 0.6251\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6558 - val_loss: 0.6188\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.6496 - val_loss: 0.6128\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.6437 - val_loss: 0.6070\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6382 - val_loss: 0.6019\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.6325 - val_loss: 0.5969\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6281 - val_loss: 0.5922\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.6234 - val_loss: 0.5877\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.6189 - val_loss: 0.5833\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6147 - val_loss: 0.5790\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.6107 - val_loss: 0.5752\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.6067 - val_loss: 0.5716\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6030 - val_loss: 0.5680\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5994 - val_loss: 0.5643\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.5960 - val_loss: 0.5606\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.5926 - val_loss: 0.5574\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5894 - val_loss: 0.5541\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5863 - val_loss: 0.5507\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5834 - val_loss: 0.5480\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5806 - val_loss: 0.5452\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5778 - val_loss: 0.5424\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5751 - val_loss: 0.5395\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5724 - val_loss: 0.5367\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.5698 - val_loss: 0.5340\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5675 - val_loss: 0.5314\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5650 - val_loss: 0.5289\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5628 - val_loss: 0.5267\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5604 - val_loss: 0.5245\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5583 - val_loss: 0.5223\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5561 - val_loss: 0.5200\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5540 - val_loss: 0.5178\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5520 - val_loss: 0.5160\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5500 - val_loss: 0.5141\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5480 - val_loss: 0.5120\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5462 - val_loss: 0.5099\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.5444 - val_loss: 0.5083\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5426 - val_loss: 0.5063\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5409 - val_loss: 0.5044\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5393 - val_loss: 0.5029\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5376 - val_loss: 0.5013\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.5361 - val_loss: 0.4997\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5344 - val_loss: 0.4978\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5329 - val_loss: 0.4965\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5314 - val_loss: 0.4950\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5300 - val_loss: 0.4938\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5286 - val_loss: 0.4925\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5273 - val_loss: 0.4909\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5254 - val_loss: 0.4894\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5246 - val_loss: 0.4881\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5233 - val_loss: 0.4869\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5218 - val_loss: 0.4852\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5207 - val_loss: 0.4845\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5195 - val_loss: 0.4835\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5183 - val_loss: 0.4823\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5171 - val_loss: 0.4812\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5159 - val_loss: 0.4797\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5148 - val_loss: 0.4786\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5138 - val_loss: 0.4776\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5127 - val_loss: 0.4767\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5117 - val_loss: 0.4758\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.5106 - val_loss: 0.4746\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5094 - val_loss: 0.4739\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5082 - val_loss: 0.4727\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5071 - val_loss: 0.4718\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5060 - val_loss: 0.4709\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.5049 - val_loss: 0.4700\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5038 - val_loss: 0.4694\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5027 - val_loss: 0.4686\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5017 - val_loss: 0.4675\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5007 - val_loss: 0.4664\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.4997 - val_loss: 0.4658\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4988 - val_loss: 0.4651\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4978 - val_loss: 0.4646\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.4969 - val_loss: 0.4638\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4960 - val_loss: 0.4630\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4951 - val_loss: 0.4618\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4943 - val_loss: 0.4614\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4935 - val_loss: 0.4603\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4927 - val_loss: 0.4596\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.4921 - val_loss: 0.4592\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.4913 - val_loss: 0.4583\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4907 - val_loss: 0.4578\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4899 - val_loss: 0.4574\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4893 - val_loss: 0.4567\n",
      "121/121 [==============================] - 0s 483us/step - loss: 0.5077\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 3.1962 - val_loss: 2.3300\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 1.9132 - val_loss: 1.5686\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 1.3655 - val_loss: 1.2104\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 1.1077 - val_loss: 1.0261\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.9759 - val_loss: 0.9254\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.9061 - val_loss: 0.8637\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.8645 - val_loss: 0.8227\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.8369 - val_loss: 0.7918\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.8151 - val_loss: 0.7675\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.7965 - val_loss: 0.7466\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.7797 - val_loss: 0.7289\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.7645 - val_loss: 0.7119\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.7507 - val_loss: 0.6969\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.7374 - val_loss: 0.6830\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.7248 - val_loss: 0.6704\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.7129 - val_loss: 0.6583\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.7019 - val_loss: 0.6468\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6917 - val_loss: 0.6359\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6817 - val_loss: 0.6259\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6722 - val_loss: 0.6169\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6631 - val_loss: 0.6083\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6546 - val_loss: 0.6008\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.6465 - val_loss: 0.5931\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6384 - val_loss: 0.5858\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.6314 - val_loss: 0.5795\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.6240 - val_loss: 0.5729\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.6174 - val_loss: 0.5670\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6110 - val_loss: 0.5622\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6051 - val_loss: 0.5574\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5991 - val_loss: 0.5523\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5937 - val_loss: 0.5475\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5888 - val_loss: 0.5436\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5839 - val_loss: 0.5407\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5787 - val_loss: 0.5368\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5746 - val_loss: 0.5334\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.5704 - val_loss: 0.5305\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5659 - val_loss: 0.5292\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.5620 - val_loss: 0.5255\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.5584 - val_loss: 0.5243\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.5549 - val_loss: 0.5227\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5512 - val_loss: 0.5195\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5482 - val_loss: 0.5188\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5452 - val_loss: 0.5174\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5423 - val_loss: 0.5157\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5393 - val_loss: 0.5145\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5367 - val_loss: 0.5135\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5336 - val_loss: 0.5117\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.5313 - val_loss: 0.5111\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5288 - val_loss: 0.5106\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5265 - val_loss: 0.5107\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5238 - val_loss: 0.5095\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5220 - val_loss: 0.5094\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5199 - val_loss: 0.5094\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5177 - val_loss: 0.5098\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5158 - val_loss: 0.5096\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5135 - val_loss: 0.5091\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5121 - val_loss: 0.5092\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5101 - val_loss: 0.5103\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5083 - val_loss: 0.5109\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5064 - val_loss: 0.5102\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5048 - val_loss: 0.5116\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5031 - val_loss: 0.5122\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5015 - val_loss: 0.5124\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5000 - val_loss: 0.5132\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4987 - val_loss: 0.5135\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4972 - val_loss: 0.5135\n",
      "121/121 [==============================] - 0s 450us/step - loss: 0.6784\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 3.2755 - val_loss: 2.2857\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 2.0138 - val_loss: 1.5582\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 1.4175 - val_loss: 1.1695\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 1.1082 - val_loss: 0.9603\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.9459 - val_loss: 0.8449\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.8581 - val_loss: 0.7779\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.8057 - val_loss: 0.7345\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.7710 - val_loss: 0.7044\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.7456 - val_loss: 0.6812\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.7253 - val_loss: 0.6624\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.7082 - val_loss: 0.6466\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.6931 - val_loss: 0.6327\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.6797 - val_loss: 0.6203\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.6677 - val_loss: 0.6092\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.6566 - val_loss: 0.5990\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.6465 - val_loss: 0.5896\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.6371 - val_loss: 0.5808\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.6284 - val_loss: 0.5727\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.6203 - val_loss: 0.5654\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6128 - val_loss: 0.5584\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.6058 - val_loss: 0.5518\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5992 - val_loss: 0.5456\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5931 - val_loss: 0.5398\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5872 - val_loss: 0.5342\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5818 - val_loss: 0.5291\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5766 - val_loss: 0.5243\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5717 - val_loss: 0.5196\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5671 - val_loss: 0.5154\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5627 - val_loss: 0.5112\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5585 - val_loss: 0.5072\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.5544 - val_loss: 0.5036\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5506 - val_loss: 0.5001\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5469 - val_loss: 0.4970\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5433 - val_loss: 0.4939\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5401 - val_loss: 0.4909\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5368 - val_loss: 0.4881\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5338 - val_loss: 0.4853\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5309 - val_loss: 0.4829\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5280 - val_loss: 0.4805\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.5254 - val_loss: 0.4781\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5228 - val_loss: 0.4757\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5204 - val_loss: 0.4736\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.5180 - val_loss: 0.4714\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.5158 - val_loss: 0.4695\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5137 - val_loss: 0.4677\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5116 - val_loss: 0.4660\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.5096 - val_loss: 0.4643\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5077 - val_loss: 0.4625\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.5059 - val_loss: 0.4609\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5041 - val_loss: 0.4595\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5024 - val_loss: 0.4580\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.5008 - val_loss: 0.4565\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4993 - val_loss: 0.4552\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4978 - val_loss: 0.4539\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4963 - val_loss: 0.4528\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4949 - val_loss: 0.4514\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4936 - val_loss: 0.4503\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4923 - val_loss: 0.4492\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4911 - val_loss: 0.4482\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4899 - val_loss: 0.4474\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.4887 - val_loss: 0.4464\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4877 - val_loss: 0.4452\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4866 - val_loss: 0.4443\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.4855 - val_loss: 0.4433\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4845 - val_loss: 0.4425\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.4835 - val_loss: 0.4417\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4826 - val_loss: 0.4409\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4816 - val_loss: 0.4402\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4808 - val_loss: 0.4393\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4799 - val_loss: 0.4387\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4791 - val_loss: 0.4379\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4783 - val_loss: 0.4373\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4775 - val_loss: 0.4368\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4767 - val_loss: 0.4360\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4760 - val_loss: 0.4354\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4753 - val_loss: 0.4348\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.4746 - val_loss: 0.4342\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.4739 - val_loss: 0.4339\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4733 - val_loss: 0.4331\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4727 - val_loss: 0.4329\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.4720 - val_loss: 0.4323\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4715 - val_loss: 0.4319\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4709 - val_loss: 0.4314\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4703 - val_loss: 0.4308\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4698 - val_loss: 0.4303\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4693 - val_loss: 0.4299\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4688 - val_loss: 0.4296\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4683 - val_loss: 0.4291\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4678 - val_loss: 0.4287\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.4674 - val_loss: 0.4282\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4669 - val_loss: 0.4279\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.4665 - val_loss: 0.4276\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4660 - val_loss: 0.4271\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4656 - val_loss: 0.4266\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4652 - val_loss: 0.4263\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4647 - val_loss: 0.4257\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4644 - val_loss: 0.4253\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4640 - val_loss: 0.4251\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4636 - val_loss: 0.4250\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4632 - val_loss: 0.4245\n",
      "121/121 [==============================] - 0s 500us/step - loss: 0.4598\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.8523 - val_loss: 0.6467\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.8030 - val_loss: 0.4775\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4819 - val_loss: 0.5275\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.5121 - val_loss: 0.5035\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.5971 - val_loss: 0.4058\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4116 - val_loss: 0.3810\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3926 - val_loss: 0.3769\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3758 - val_loss: 0.3581\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3659 - val_loss: 0.3588\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.3566 - val_loss: 0.3574\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 797us/step - loss: 0.3472 - val_loss: 0.3466\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3409 - val_loss: 0.3459\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3389 - val_loss: 0.3701\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3343 - val_loss: 0.3388\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3266 - val_loss: 0.3312\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3245 - val_loss: 0.3235\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3195 - val_loss: 0.3251\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3200 - val_loss: 0.3351\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3135 - val_loss: 0.3217\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3080 - val_loss: 0.3251\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3073 - val_loss: 0.3734\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3089 - val_loss: 0.3170\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3012 - val_loss: 0.3419\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2995 - val_loss: 0.3300\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3002 - val_loss: 0.3564\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.2987 - val_loss: 0.3150\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2940 - val_loss: 0.3081\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.2920 - val_loss: 0.3131\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.2911 - val_loss: 0.3044\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2869 - val_loss: 0.2973\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2906 - val_loss: 0.3058\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2861 - val_loss: 0.3165\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2855 - val_loss: 0.2933\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2846 - val_loss: 0.3007\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.2821 - val_loss: 0.3104\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.2799 - val_loss: 0.3072\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2822 - val_loss: 0.3201\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.2787 - val_loss: 0.2921\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2800 - val_loss: 0.2987\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.2778 - val_loss: 0.2973\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.2777 - val_loss: 0.3153\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.2743 - val_loss: 0.3227\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2725 - val_loss: 0.3172\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.2743 - val_loss: 0.2916\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.2742 - val_loss: 0.3038\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.2779 - val_loss: 0.2930\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.2695 - val_loss: 0.3108\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.2705 - val_loss: 0.2914\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2690 - val_loss: 0.2835\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2688 - val_loss: 0.3045\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.2712 - val_loss: 0.2922\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.2656 - val_loss: 0.2947\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2680 - val_loss: 0.2876\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2641 - val_loss: 0.2919\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.2658 - val_loss: 0.2818\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.2635 - val_loss: 0.2876\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2628 - val_loss: 0.2991\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.2631 - val_loss: 0.3026\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.2619 - val_loss: 0.3458\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.2634 - val_loss: 0.2920\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2616 - val_loss: 0.3077\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.2594 - val_loss: 0.2881\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2583 - val_loss: 0.3093\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.2574 - val_loss: 0.2862\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.2591 - val_loss: 0.2841\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.2982\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7928 - val_loss: 0.5732\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.5302 - val_loss: 0.4371\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.4468 - val_loss: 0.3930\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4182 - val_loss: 0.3888\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4029 - val_loss: 0.3818\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3931 - val_loss: 0.3715\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3789 - val_loss: 0.3873\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3699 - val_loss: 0.3631\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3626 - val_loss: 0.3645\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3562 - val_loss: 0.3544\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.3489 - val_loss: 0.3496\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3454 - val_loss: 0.3505\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.3406 - val_loss: 0.3527\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3364 - val_loss: 0.3570\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3306 - val_loss: 0.3497\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3298 - val_loss: 0.3383\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3267 - val_loss: 0.3251\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3178 - val_loss: 0.3279\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3164 - val_loss: 0.3485\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.3158 - val_loss: 0.3100\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.3107 - val_loss: 0.3192\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.3041 - val_loss: 0.3331\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3082 - val_loss: 0.3119\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3088 - val_loss: 0.3154\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3051 - val_loss: 0.3181\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.3041 - val_loss: 0.3253\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3000 - val_loss: 0.3102\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.2987 - val_loss: 0.3064\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.2964 - val_loss: 0.3187\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.2968 - val_loss: 0.3226\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.2960 - val_loss: 0.3085\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.2929 - val_loss: 0.3371\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.2952 - val_loss: 0.3369\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.2884 - val_loss: 0.3217\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.2883 - val_loss: 0.3364\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2917 - val_loss: 0.3438\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.2883 - val_loss: 0.3550\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.2863 - val_loss: 0.3358\n",
      "121/121 [==============================] - 0s 483us/step - loss: 0.3703\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 1.2005 - val_loss: 1.6155\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 459us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 2.9160 - val_loss: 1.3610\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 1.1132 - val_loss: 0.9049\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.8513 - val_loss: 0.7779\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.7649 - val_loss: 0.7216\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.7260 - val_loss: 0.6862\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.6976 - val_loss: 0.6573\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.6730 - val_loss: 0.6331\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.6509 - val_loss: 0.6122\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.6312 - val_loss: 0.5916\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.6129 - val_loss: 0.5737\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5964 - val_loss: 0.5577\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5812 - val_loss: 0.5432\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5674 - val_loss: 0.5303\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5549 - val_loss: 0.5187\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.5438 - val_loss: 0.5077\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.5334 - val_loss: 0.4984\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5242 - val_loss: 0.4886\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.5161 - val_loss: 0.4821\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.5086 - val_loss: 0.4746\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5015 - val_loss: 0.4674\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.4955 - val_loss: 0.4619\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4896 - val_loss: 0.4578\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4845 - val_loss: 0.4520\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4799 - val_loss: 0.4479\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.4753 - val_loss: 0.4456\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4715 - val_loss: 0.4412\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4677 - val_loss: 0.4377\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.4642 - val_loss: 0.4340\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.4612 - val_loss: 0.4316\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4581 - val_loss: 0.4285\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4551 - val_loss: 0.4267\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.4526 - val_loss: 0.4257\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4503 - val_loss: 0.4233\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4479 - val_loss: 0.4199\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4456 - val_loss: 0.4179\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.4434 - val_loss: 0.4174\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.4416 - val_loss: 0.4144\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4398 - val_loss: 0.4139\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4378 - val_loss: 0.4110\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4362 - val_loss: 0.4099\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4346 - val_loss: 0.4088\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4328 - val_loss: 0.4088\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4313 - val_loss: 0.4064\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4299 - val_loss: 0.4062\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.4284 - val_loss: 0.4045\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4269 - val_loss: 0.4041\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.4258 - val_loss: 0.4032\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4244 - val_loss: 0.4009\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4231 - val_loss: 0.3998\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4218 - val_loss: 0.4004\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4206 - val_loss: 0.4000\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4196 - val_loss: 0.3983\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4183 - val_loss: 0.3980\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4173 - val_loss: 0.3965\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.4159 - val_loss: 0.3959\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.4150 - val_loss: 0.3955\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4139 - val_loss: 0.3932\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4127 - val_loss: 0.3917\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.4119 - val_loss: 0.3924\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4106 - val_loss: 0.3927\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4098 - val_loss: 0.3921\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4092 - val_loss: 0.3903\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4080 - val_loss: 0.3901\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4068 - val_loss: 0.3875\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4063 - val_loss: 0.3887\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4054 - val_loss: 0.3874\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4045 - val_loss: 0.3873\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4037 - val_loss: 0.3870\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4027 - val_loss: 0.3854\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4015 - val_loss: 0.3841\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.4013 - val_loss: 0.3844\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4002 - val_loss: 0.3854\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3993 - val_loss: 0.3833\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3986 - val_loss: 0.3834\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3976 - val_loss: 0.3834\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3971 - val_loss: 0.3824\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3961 - val_loss: 0.3805\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.3951 - val_loss: 0.3812\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.3946 - val_loss: 0.3796\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3938 - val_loss: 0.3800\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3932 - val_loss: 0.3785\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3924 - val_loss: 0.3784\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3916 - val_loss: 0.3773\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3909 - val_loss: 0.3773\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3900 - val_loss: 0.3773\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3893 - val_loss: 0.3768\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3886 - val_loss: 0.3761\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3880 - val_loss: 0.3755\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3872 - val_loss: 0.3755\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3865 - val_loss: 0.3739\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3859 - val_loss: 0.3735\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3851 - val_loss: 0.3733\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3841 - val_loss: 0.3738\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3838 - val_loss: 0.3733\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3829 - val_loss: 0.3715\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3825 - val_loss: 0.3720\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3817 - val_loss: 0.3714\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3808 - val_loss: 0.3728\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3806 - val_loss: 0.3703\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.3798 - val_loss: 0.3702\n",
      "121/121 [==============================] - 0s 446us/step - loss: 0.4062\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 2.9886 - val_loss: 1.2924\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 1.0058 - val_loss: 0.8272\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.7894 - val_loss: 0.7115\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.7243 - val_loss: 0.6590\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6880 - val_loss: 0.6246\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.6622 - val_loss: 0.5999\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.6415 - val_loss: 0.5809\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.6240 - val_loss: 0.5650\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.6087 - val_loss: 0.5520\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5948 - val_loss: 0.5411\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5828 - val_loss: 0.5301\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.5717 - val_loss: 0.5225\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5621 - val_loss: 0.5143\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5529 - val_loss: 0.5079\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5445 - val_loss: 0.4999\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5370 - val_loss: 0.4941\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5302 - val_loss: 0.4900\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5234 - val_loss: 0.4849\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5176 - val_loss: 0.4803\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5118 - val_loss: 0.4778\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5072 - val_loss: 0.4741\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5019 - val_loss: 0.4694\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4970 - val_loss: 0.4691\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4936 - val_loss: 0.4661\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4898 - val_loss: 0.4626\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4847 - val_loss: 0.4589\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4815 - val_loss: 0.4553\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4783 - val_loss: 0.4544\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4747 - val_loss: 0.4518\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4729 - val_loss: 0.4507\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4693 - val_loss: 0.4484\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4673 - val_loss: 0.4480\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4640 - val_loss: 0.4484\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4608 - val_loss: 0.4441\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.4600 - val_loss: 0.4435\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4576 - val_loss: 0.4441\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4561 - val_loss: 0.4414\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.4527 - val_loss: 0.4397\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4519 - val_loss: 0.4388\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4497 - val_loss: 0.4379\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4469 - val_loss: 0.4377\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4457 - val_loss: 0.4365\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4439 - val_loss: 0.4373\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4417 - val_loss: 0.4356\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4403 - val_loss: 0.4334\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4398 - val_loss: 0.4331\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4371 - val_loss: 0.4330\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4363 - val_loss: 0.4330\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4342 - val_loss: 0.4323\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.4323 - val_loss: 0.4310\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4314 - val_loss: 0.4302\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4295 - val_loss: 0.4321\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4286 - val_loss: 0.4304\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4269 - val_loss: 0.4302\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4255 - val_loss: 0.4311\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.4245 - val_loss: 0.4294\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.4231 - val_loss: 0.4287\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4215 - val_loss: 0.4286\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4206 - val_loss: 0.4288\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4192 - val_loss: 0.4269\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4180 - val_loss: 0.4293\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4169 - val_loss: 0.4273\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4157 - val_loss: 0.4271\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4145 - val_loss: 0.4261\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4137 - val_loss: 0.4263\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.4122 - val_loss: 0.4254\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.4115 - val_loss: 0.4255\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4102 - val_loss: 0.4268\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.4094 - val_loss: 0.4268\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4083 - val_loss: 0.4252\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4073 - val_loss: 0.4260\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4063 - val_loss: 0.4264\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4051 - val_loss: 0.4256\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4042 - val_loss: 0.4263\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4035 - val_loss: 0.4245\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.4025 - val_loss: 0.4246\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4017 - val_loss: 0.4248\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4005 - val_loss: 0.4251\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3999 - val_loss: 0.4242\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3986 - val_loss: 0.4232\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3982 - val_loss: 0.4249\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3971 - val_loss: 0.4244\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3961 - val_loss: 0.4233\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3954 - val_loss: 0.4245\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3946 - val_loss: 0.4235\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3938 - val_loss: 0.4235\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3929 - val_loss: 0.4244\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.3921 - val_loss: 0.4243\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.3914 - val_loss: 0.4243\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3906 - val_loss: 0.4237\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.5659\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 3.3530 - val_loss: 1.5411\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 1.2304 - val_loss: 0.9623\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.8998 - val_loss: 0.8030\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.8094 - val_loss: 0.7328\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.7611 - val_loss: 0.6897\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.7272 - val_loss: 0.6586\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.6993 - val_loss: 0.6342\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.6765 - val_loss: 0.6127\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.6558 - val_loss: 0.5939\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.6372 - val_loss: 0.5777\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.6215 - val_loss: 0.5636\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.6068 - val_loss: 0.5508\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.5934 - val_loss: 0.5386\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5818 - val_loss: 0.5280\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5707 - val_loss: 0.5189\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5613 - val_loss: 0.5105\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.5522 - val_loss: 0.5025\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.5441 - val_loss: 0.4953\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5367 - val_loss: 0.4890\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5299 - val_loss: 0.4827\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.5236 - val_loss: 0.4782\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.5180 - val_loss: 0.4727\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5125 - val_loss: 0.4679\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.5072 - val_loss: 0.4635\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5028 - val_loss: 0.4600\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4986 - val_loss: 0.4561\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4944 - val_loss: 0.4529\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4907 - val_loss: 0.4500\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4870 - val_loss: 0.4466\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4834 - val_loss: 0.4441\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4804 - val_loss: 0.4412\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4774 - val_loss: 0.4389\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4745 - val_loss: 0.4363\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.4718 - val_loss: 0.4346\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4692 - val_loss: 0.4320\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4670 - val_loss: 0.4303\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4647 - val_loss: 0.4287\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.4625 - val_loss: 0.4268\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4604 - val_loss: 0.4251\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4585 - val_loss: 0.4233\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4564 - val_loss: 0.4222\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.4546 - val_loss: 0.4209\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4527 - val_loss: 0.4200\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4511 - val_loss: 0.4178\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.4494 - val_loss: 0.4164\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4479 - val_loss: 0.4153\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4461 - val_loss: 0.4139\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.4444 - val_loss: 0.4134\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.4433 - val_loss: 0.4120\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4416 - val_loss: 0.4102\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4402 - val_loss: 0.4099\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4391 - val_loss: 0.4095\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.4375 - val_loss: 0.4086\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4361 - val_loss: 0.4076\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4351 - val_loss: 0.4058\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4339 - val_loss: 0.4045\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4327 - val_loss: 0.4039\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.4314 - val_loss: 0.4030\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4303 - val_loss: 0.4023\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4292 - val_loss: 0.4014\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4282 - val_loss: 0.4008\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4271 - val_loss: 0.3996\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4258 - val_loss: 0.3999\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.4251 - val_loss: 0.3989\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4239 - val_loss: 0.3982\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4231 - val_loss: 0.3971\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4220 - val_loss: 0.3961\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4209 - val_loss: 0.3960\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4202 - val_loss: 0.3948\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.4189 - val_loss: 0.3943\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4180 - val_loss: 0.3949\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4174 - val_loss: 0.3934\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4161 - val_loss: 0.3929\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4154 - val_loss: 0.3916\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4145 - val_loss: 0.3920\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.4137 - val_loss: 0.3904\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4127 - val_loss: 0.3905\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4120 - val_loss: 0.3897\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.4110 - val_loss: 0.3889\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4103 - val_loss: 0.3882\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4096 - val_loss: 0.3875\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4087 - val_loss: 0.3874\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.4080 - val_loss: 0.3875\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.4073 - val_loss: 0.3871\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.4064 - val_loss: 0.3863\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4057 - val_loss: 0.3854\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4049 - val_loss: 0.3856\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.4041 - val_loss: 0.3845\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4035 - val_loss: 0.3841\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4029 - val_loss: 0.3835\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4020 - val_loss: 0.3831\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.4013 - val_loss: 0.3823\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4004 - val_loss: 0.3822\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3999 - val_loss: 0.3814\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3992 - val_loss: 0.3813\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3983 - val_loss: 0.3807\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3979 - val_loss: 0.3807\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3971 - val_loss: 0.3804\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3963 - val_loss: 0.3808\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3957 - val_loss: 0.3785\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.3979\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 1.8122 - val_loss: 1.2556\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 1.8901 - val_loss: 0.6118\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.6305 - val_loss: 0.4986\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5022 - val_loss: 0.4617\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4730 - val_loss: 0.4416\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4506 - val_loss: 0.4200\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.4322 - val_loss: 0.4036\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4189 - val_loss: 0.3986\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4102 - val_loss: 0.3919\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4037 - val_loss: 0.3834\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3961 - val_loss: 0.3796\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3902 - val_loss: 0.3752\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3853 - val_loss: 0.3721\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3802 - val_loss: 0.3681\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3751 - val_loss: 0.3726\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3724 - val_loss: 0.3652\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3680 - val_loss: 0.3623\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3652 - val_loss: 0.3606\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3610 - val_loss: 0.3554\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3594 - val_loss: 0.3544\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3556 - val_loss: 0.3513\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.3527 - val_loss: 0.3538\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3505 - val_loss: 0.3504\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3475 - val_loss: 0.3479\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3444 - val_loss: 0.3493\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3426 - val_loss: 0.3442\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3414 - val_loss: 0.3432\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3396 - val_loss: 0.3418\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3372 - val_loss: 0.3416\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3352 - val_loss: 0.3423\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.3323 - val_loss: 0.3423\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3304 - val_loss: 0.3386\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3288 - val_loss: 0.3327\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3269 - val_loss: 0.3341\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3267 - val_loss: 0.3319\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3249 - val_loss: 0.3407\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3224 - val_loss: 0.3321\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3223 - val_loss: 0.3286\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3196 - val_loss: 0.3279\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.3186 - val_loss: 0.3286\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3175 - val_loss: 0.3264\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3162 - val_loss: 0.3244\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3156 - val_loss: 0.3266\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3139 - val_loss: 0.3223\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3120 - val_loss: 0.3208\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3116 - val_loss: 0.3258\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3094 - val_loss: 0.3219\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3090 - val_loss: 0.3215\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.3069 - val_loss: 0.3202\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3053 - val_loss: 0.3244\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3041 - val_loss: 0.3249\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.3033 - val_loss: 0.3246\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3031 - val_loss: 0.3157\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3018 - val_loss: 0.3173\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3016 - val_loss: 0.3152\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2985 - val_loss: 0.3174\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.2993 - val_loss: 0.3141\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.2981 - val_loss: 0.3114\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.2970 - val_loss: 0.3105\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.2959 - val_loss: 0.3115\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2959 - val_loss: 0.3118\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2947 - val_loss: 0.3086\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.2931 - val_loss: 0.3113\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2928 - val_loss: 0.3074\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2917 - val_loss: 0.3050\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.2910 - val_loss: 0.3083\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.2915 - val_loss: 0.3074\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2883 - val_loss: 0.3057\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.2898 - val_loss: 0.3075\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2880 - val_loss: 0.3056\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2866 - val_loss: 0.3055\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2853 - val_loss: 0.3065\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.2856 - val_loss: 0.3107\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.2851 - val_loss: 0.3016\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.2839 - val_loss: 0.3033\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.2828 - val_loss: 0.3036\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.2821 - val_loss: 0.3084\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2817 - val_loss: 0.3028\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.2814 - val_loss: 0.3061\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.2790 - val_loss: 0.3024\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2808 - val_loss: 0.3032\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2787 - val_loss: 0.3039\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.2777 - val_loss: 0.3003\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2775 - val_loss: 0.3016\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.2766 - val_loss: 0.2988\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2767 - val_loss: 0.3029\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2762 - val_loss: 0.3002\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2762 - val_loss: 0.3070\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.2755 - val_loss: 0.2985\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.2742 - val_loss: 0.3037\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2736 - val_loss: 0.3065\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.2743 - val_loss: 0.2972\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2731 - val_loss: 0.2992\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2722 - val_loss: 0.3064\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2742 - val_loss: 0.3006\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2725 - val_loss: 0.2965\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2713 - val_loss: 0.2957\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2713 - val_loss: 0.2993\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.2694 - val_loss: 0.2960\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.2709 - val_loss: 0.3014\n",
      "121/121 [==============================] - 0s 450us/step - loss: 0.3062\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 1.0860 - val_loss: 0.7251\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.6287 - val_loss: 0.5944\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.5574 - val_loss: 0.5148\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.5162 - val_loss: 0.4733\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.4876 - val_loss: 0.4434\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.4660 - val_loss: 0.4252\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4569 - val_loss: 0.4145\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4417 - val_loss: 0.4094\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.4317 - val_loss: 0.4070\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4229 - val_loss: 0.3942\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4159 - val_loss: 0.3908\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.4101 - val_loss: 0.3881\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.4051 - val_loss: 0.3830\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.3996 - val_loss: 0.3811\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3954 - val_loss: 0.3795\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3905 - val_loss: 0.3762\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3909 - val_loss: 0.3770\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3827 - val_loss: 0.3731\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3787 - val_loss: 0.3708\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3766 - val_loss: 0.3706\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.3730 - val_loss: 0.3643\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3716 - val_loss: 0.3622\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3704 - val_loss: 0.3610\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3638 - val_loss: 0.3583\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3631 - val_loss: 0.3573\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3589 - val_loss: 0.3575\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3572 - val_loss: 0.3512\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3620 - val_loss: 0.3531\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3527 - val_loss: 0.3571\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.3511 - val_loss: 0.3514\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3498 - val_loss: 0.3612\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3495 - val_loss: 0.3477\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3490 - val_loss: 0.3460\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.3437 - val_loss: 0.3459\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3417 - val_loss: 0.3444\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3398 - val_loss: 0.3409\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3391 - val_loss: 0.3371\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3382 - val_loss: 0.3362\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3362 - val_loss: 0.3376\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3348 - val_loss: 0.3356\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3387 - val_loss: 0.3335\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3307 - val_loss: 0.3434\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3290 - val_loss: 0.3296\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3300 - val_loss: 0.3303\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3263 - val_loss: 0.3281\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3249 - val_loss: 0.3284\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3242 - val_loss: 0.3284\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3353 - val_loss: 0.3292\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3208 - val_loss: 0.3301\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3198 - val_loss: 0.3262\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3177 - val_loss: 0.3246\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3162 - val_loss: 0.3260\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3175 - val_loss: 0.3288\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3134 - val_loss: 0.3248\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3141 - val_loss: 0.3261\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3214 - val_loss: 0.3249\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3115 - val_loss: 0.3216\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3095 - val_loss: 0.3248\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3081 - val_loss: 0.3296\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3089 - val_loss: 0.3232\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3082 - val_loss: 0.3229\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3056 - val_loss: 0.3193\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3040 - val_loss: 0.3353\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3032 - val_loss: 0.3278\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.3018 - val_loss: 0.3269\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3003 - val_loss: 0.3224\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2997 - val_loss: 0.3227\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3032 - val_loss: 0.3242\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2994 - val_loss: 0.3214\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2985 - val_loss: 0.3251\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.2983 - val_loss: 0.3235\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2980 - val_loss: 0.3213\n",
      "121/121 [==============================] - 0s 450us/step - loss: 0.3831\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 1.1474 - val_loss: 0.9042\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.9260 - val_loss: 0.5590\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.5691 - val_loss: 0.4975\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.5229 - val_loss: 0.4607\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4916 - val_loss: 0.4402\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4709 - val_loss: 0.4243\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4559 - val_loss: 0.4136\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4439 - val_loss: 0.4045\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.4350 - val_loss: 0.4035\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4273 - val_loss: 0.3941\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4210 - val_loss: 0.3913\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4158 - val_loss: 0.3856\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.4100 - val_loss: 0.3864\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4045 - val_loss: 0.3791\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4001 - val_loss: 0.3751\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3967 - val_loss: 0.3738\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3919 - val_loss: 0.3698\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3879 - val_loss: 0.3740\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3846 - val_loss: 0.3667\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.3812 - val_loss: 0.3638\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3783 - val_loss: 0.3610\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3739 - val_loss: 0.3730\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3718 - val_loss: 0.3586\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3697 - val_loss: 0.3575\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3661 - val_loss: 0.3559\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3635 - val_loss: 0.3554\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3615 - val_loss: 0.3544\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3600 - val_loss: 0.3513\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3577 - val_loss: 0.3499\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3534 - val_loss: 0.3492\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3527 - val_loss: 0.3493\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3500 - val_loss: 0.3463\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3489 - val_loss: 0.3432\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3458 - val_loss: 0.3444\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3440 - val_loss: 0.3415\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3419 - val_loss: 0.3401\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3407 - val_loss: 0.3395\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3390 - val_loss: 0.3386\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3374 - val_loss: 0.3376\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3355 - val_loss: 0.3378\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3344 - val_loss: 0.3375\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3316 - val_loss: 0.3423\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3311 - val_loss: 0.3329\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3291 - val_loss: 0.3399\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3274 - val_loss: 0.3327\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3281 - val_loss: 0.3314\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3277 - val_loss: 0.3313\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3266 - val_loss: 0.3278\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3226 - val_loss: 0.3313\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3213 - val_loss: 0.3328\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3200 - val_loss: 0.3272\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3193 - val_loss: 0.3271\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3170 - val_loss: 0.3273\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3170 - val_loss: 0.3327\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3148 - val_loss: 0.3243\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3139 - val_loss: 0.3223\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3132 - val_loss: 0.3208\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3112 - val_loss: 0.3196\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3106 - val_loss: 0.3249\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3106 - val_loss: 0.3189\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3082 - val_loss: 0.3190\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.3081 - val_loss: 0.3185\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3071 - val_loss: 0.3189\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3057 - val_loss: 0.3199\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3053 - val_loss: 0.3235\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3033 - val_loss: 0.3217\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3029 - val_loss: 0.3150\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3016 - val_loss: 0.3175\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.3016 - val_loss: 0.3133\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3003 - val_loss: 0.3125\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.2995 - val_loss: 0.3164\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2982 - val_loss: 0.3169\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2978 - val_loss: 0.3122\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.2969 - val_loss: 0.3102\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2953 - val_loss: 0.3156\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2960 - val_loss: 0.3136\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.2955 - val_loss: 0.3133\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.2948 - val_loss: 0.3086\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.2938 - val_loss: 0.3143\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.2933 - val_loss: 0.3100\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2916 - val_loss: 0.3081\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2912 - val_loss: 0.3108\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2896 - val_loss: 0.3165\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.2904 - val_loss: 0.3087\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2901 - val_loss: 0.3085\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.2902 - val_loss: 0.3091\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.2889 - val_loss: 0.3091\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.2886 - val_loss: 0.3077\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2891 - val_loss: 0.3095\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2887 - val_loss: 0.3066\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.2873 - val_loss: 0.3067\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.2864 - val_loss: 0.3078\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.2853 - val_loss: 0.3099\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.2842 - val_loss: 0.3074\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2832 - val_loss: 0.3079\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.2829 - val_loss: 0.3055\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2827 - val_loss: 0.3038\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2830 - val_loss: 0.3029\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.2812 - val_loss: 0.3042\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.2815 - val_loss: 0.3105\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.2988\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 3.9844 - val_loss: 2.8038\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 2.2647 - val_loss: 1.8464\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 1.6597 - val_loss: 1.5134\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 1.4462 - val_loss: 1.3979\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 1.3708 - val_loss: 1.3584\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 1.3440 - val_loss: 1.3450\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 1.3344 - val_loss: 1.3407\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.3310 - val_loss: 1.3395\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 1.3299 - val_loss: 1.3393\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 1.3295 - val_loss: 1.3393\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 1.3293 - val_loss: 1.3394\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 1.3293 - val_loss: 1.3395\n",
      "121/121 [==============================] - 0s 458us/step - loss: 1.3688\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0237 - val_loss: 2.7957\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 2.2930 - val_loss: 1.8396\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 1.6832 - val_loss: 1.5084\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.4680 - val_loss: 1.3946\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 1.3919 - val_loss: 1.3563\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 1.3649 - val_loss: 1.3439\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 1.3554 - val_loss: 1.3402\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 1.3520 - val_loss: 1.3394\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 1.3508 - val_loss: 1.3393\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 1.3505 - val_loss: 1.3394\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 1.3503 - val_loss: 1.3396\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 1.3503 - val_loss: 1.3397\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 1.3502 - val_loss: 1.3398\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 1.3502 - val_loss: 1.3398\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 1.3503 - val_loss: 1.3399\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 1.3502 - val_loss: 1.3398\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 1.3503 - val_loss: 1.3398\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 1.3502 - val_loss: 1.3397\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 1.3502 - val_loss: 1.3397\n",
      "121/121 [==============================] - 0s 550us/step - loss: 1.3267\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.0347 - val_loss: 2.7969\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 2.2975 - val_loss: 1.8397\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 1.6830 - val_loss: 1.5059\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 1.4653 - val_loss: 1.3925\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 1.3892 - val_loss: 1.3552\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 1.3625 - val_loss: 1.3435\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 1.3529 - val_loss: 1.3400\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 1.3495 - val_loss: 1.3393\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 1.3484 - val_loss: 1.3393\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 1.3479 - val_loss: 1.3395\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 1.3478 - val_loss: 1.3397\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 1.3477 - val_loss: 1.3398\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 1.3477 - val_loss: 1.3399\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 1.3477 - val_loss: 1.3399\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 1.3477 - val_loss: 1.3400\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 1.3477 - val_loss: 1.3400\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 1.3477 - val_loss: 1.3400\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 1.3477 - val_loss: 1.3400\n",
      "121/121 [==============================] - 0s 467us/step - loss: 1.3319\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 2.8557 - val_loss: 1.7037\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 1.3411 - val_loss: 1.0442\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.9257 - val_loss: 0.8428\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.7998 - val_loss: 0.7736\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.7496 - val_loss: 0.7385\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.7262 - val_loss: 0.7159\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.7103 - val_loss: 0.6982\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.6976 - val_loss: 0.6838\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.6864 - val_loss: 0.6709\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6763 - val_loss: 0.6598\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.6670 - val_loss: 0.6495\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6582 - val_loss: 0.6396\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.6499 - val_loss: 0.6303\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.6419 - val_loss: 0.6214\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.6343 - val_loss: 0.6133\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6273 - val_loss: 0.6057\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.6203 - val_loss: 0.5986\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6138 - val_loss: 0.5910\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6075 - val_loss: 0.5846\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.6016 - val_loss: 0.5777\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5960 - val_loss: 0.5713\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5904 - val_loss: 0.5657\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5853 - val_loss: 0.5597\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.5805 - val_loss: 0.5543\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5756 - val_loss: 0.5493\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5709 - val_loss: 0.5446\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5667 - val_loss: 0.5395\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5623 - val_loss: 0.5353\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5585 - val_loss: 0.5307\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5544 - val_loss: 0.5261\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.5508 - val_loss: 0.5220\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5473 - val_loss: 0.5187\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.5440 - val_loss: 0.5146\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5407 - val_loss: 0.5110\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5376 - val_loss: 0.5081\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.5345 - val_loss: 0.5045\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.5318 - val_loss: 0.5015\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5290 - val_loss: 0.4991\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5263 - val_loss: 0.4960\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5237 - val_loss: 0.4934\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.5214 - val_loss: 0.4912\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5190 - val_loss: 0.4884\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5167 - val_loss: 0.4862\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5146 - val_loss: 0.4848\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5125 - val_loss: 0.4822\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5106 - val_loss: 0.4799\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5087 - val_loss: 0.4777\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5068 - val_loss: 0.4766\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5052 - val_loss: 0.4749\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.5035 - val_loss: 0.4733\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.5019 - val_loss: 0.4712\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.5001 - val_loss: 0.4704\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.4989 - val_loss: 0.4683\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4973 - val_loss: 0.4674\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4958 - val_loss: 0.4654\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4946 - val_loss: 0.4645\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4932 - val_loss: 0.4625\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4920 - val_loss: 0.4611\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4909 - val_loss: 0.4604\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4896 - val_loss: 0.4595\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.4885 - val_loss: 0.4582\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4873 - val_loss: 0.4574\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4862 - val_loss: 0.4556\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4852 - val_loss: 0.4551\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4842 - val_loss: 0.4538\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4830 - val_loss: 0.4526\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4821 - val_loss: 0.4513\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.4812 - val_loss: 0.4505\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4802 - val_loss: 0.4493\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4795 - val_loss: 0.4490\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4785 - val_loss: 0.4483\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4777 - val_loss: 0.4471\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4768 - val_loss: 0.4465\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4758 - val_loss: 0.4453\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4753 - val_loss: 0.4446\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4744 - val_loss: 0.4444\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4735 - val_loss: 0.4430\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4728 - val_loss: 0.4430\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4720 - val_loss: 0.4422\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4714 - val_loss: 0.4412\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4705 - val_loss: 0.4403\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4698 - val_loss: 0.4403\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4691 - val_loss: 0.4390\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4685 - val_loss: 0.4384\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4678 - val_loss: 0.4378\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4672 - val_loss: 0.4373\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4665 - val_loss: 0.4366\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4657 - val_loss: 0.4358\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.4650 - val_loss: 0.4351\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4641 - val_loss: 0.4350\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.4633 - val_loss: 0.4342\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4625 - val_loss: 0.4331\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4618 - val_loss: 0.4328\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.4609 - val_loss: 0.4329\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4604 - val_loss: 0.4322\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.4596 - val_loss: 0.4314\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4589 - val_loss: 0.4314\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.4583 - val_loss: 0.4304\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.4576 - val_loss: 0.4298\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.4571 - val_loss: 0.4293\n",
      "121/121 [==============================] - 0s 442us/step - loss: 0.4704\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 4.9990 - val_loss: 2.7962\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 2.0358 - val_loss: 1.4961\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 1.2779 - val_loss: 1.1000\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 1.0129 - val_loss: 0.9385\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.8952 - val_loss: 0.8522\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.8313 - val_loss: 0.7985\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.7914 - val_loss: 0.7626\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.7640 - val_loss: 0.7368\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.7430 - val_loss: 0.7163\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.7261 - val_loss: 0.7000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.7117 - val_loss: 0.6861\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.6990 - val_loss: 0.6743\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.6877 - val_loss: 0.6638\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6773 - val_loss: 0.6542\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6676 - val_loss: 0.6455\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6585 - val_loss: 0.6375\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.6499 - val_loss: 0.6301\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.6419 - val_loss: 0.6231\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.6341 - val_loss: 0.6165\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6269 - val_loss: 0.6104\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6200 - val_loss: 0.6044\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.6134 - val_loss: 0.5989\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.6070 - val_loss: 0.5939\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6010 - val_loss: 0.5891\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.5951 - val_loss: 0.5843\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.5897 - val_loss: 0.5801\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5844 - val_loss: 0.5762\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5794 - val_loss: 0.5720\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5746 - val_loss: 0.5684\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5700 - val_loss: 0.5649\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.5655 - val_loss: 0.5616\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5611 - val_loss: 0.5591\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5573 - val_loss: 0.5558\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5535 - val_loss: 0.5530\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5497 - val_loss: 0.5503\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5463 - val_loss: 0.5475\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5429 - val_loss: 0.5452\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.5397 - val_loss: 0.5432\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5366 - val_loss: 0.5413\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5337 - val_loss: 0.5391\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5308 - val_loss: 0.5379\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.5282 - val_loss: 0.5361\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5257 - val_loss: 0.5344\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5233 - val_loss: 0.5331\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5209 - val_loss: 0.5313\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.5186 - val_loss: 0.5294\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5166 - val_loss: 0.5283\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5145 - val_loss: 0.5278\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.5125 - val_loss: 0.5262\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.5106 - val_loss: 0.5255\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.5088 - val_loss: 0.5244\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.5070 - val_loss: 0.5240\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5051 - val_loss: 0.5222\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5037 - val_loss: 0.5224\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5020 - val_loss: 0.5207\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.5011 - val_loss: 0.5202\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4995 - val_loss: 0.5195\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4981 - val_loss: 0.5187\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4968 - val_loss: 0.5188\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4954 - val_loss: 0.5173\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.4942 - val_loss: 0.5180\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4934 - val_loss: 0.5171\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4919 - val_loss: 0.5160\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4911 - val_loss: 0.5152\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4899 - val_loss: 0.5158\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4890 - val_loss: 0.5153\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.4879 - val_loss: 0.5146\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4867 - val_loss: 0.5149\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.4858 - val_loss: 0.5134\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.4850 - val_loss: 0.5128\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4841 - val_loss: 0.5121\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4831 - val_loss: 0.5124\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.4821 - val_loss: 0.5115\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.4812 - val_loss: 0.5120\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4804 - val_loss: 0.5118\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.4797 - val_loss: 0.5107\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4787 - val_loss: 0.5102\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.4781 - val_loss: 0.5099\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.4773 - val_loss: 0.5094\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.4761 - val_loss: 0.5085\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4758 - val_loss: 0.5083\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4750 - val_loss: 0.5089\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.4742 - val_loss: 0.5088\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4735 - val_loss: 0.5075\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4728 - val_loss: 0.5073\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.4720 - val_loss: 0.5065\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4713 - val_loss: 0.5077\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4705 - val_loss: 0.5082\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4703 - val_loss: 0.5073\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.4695 - val_loss: 0.5064\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.4687 - val_loss: 0.5072\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4683 - val_loss: 0.5053\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.4675 - val_loss: 0.5057\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4667 - val_loss: 0.5046\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.4664 - val_loss: 0.5045\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.4657 - val_loss: 0.5046\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4649 - val_loss: 0.5039\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.4646 - val_loss: 0.5038\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.4639 - val_loss: 0.5043\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4635 - val_loss: 0.5034\n",
      "121/121 [==============================] - 0s 458us/step - loss: 0.6933\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 3.3504 - val_loss: 1.9859\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 1.5427 - val_loss: 1.1681\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.0680 - val_loss: 0.9245\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.8961 - val_loss: 0.8207\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.8269 - val_loss: 0.7671\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.7912 - val_loss: 0.7334\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.7673 - val_loss: 0.7091\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.7487 - val_loss: 0.6903\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.7329 - val_loss: 0.6743\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.7190 - val_loss: 0.6601\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.7062 - val_loss: 0.6477\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.6946 - val_loss: 0.6362\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.6835 - val_loss: 0.6258\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.6735 - val_loss: 0.6165\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.6640 - val_loss: 0.6072\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.6549 - val_loss: 0.5986\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.6464 - val_loss: 0.5911\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6385 - val_loss: 0.5835\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.6309 - val_loss: 0.5764\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.6236 - val_loss: 0.5695\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.6167 - val_loss: 0.5631\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.6101 - val_loss: 0.5567\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.6039 - val_loss: 0.5510\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5979 - val_loss: 0.5453\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5923 - val_loss: 0.5401\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5868 - val_loss: 0.5353\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5817 - val_loss: 0.5306\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.5769 - val_loss: 0.5261\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5723 - val_loss: 0.5217\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5678 - val_loss: 0.5180\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5636 - val_loss: 0.5140\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5596 - val_loss: 0.5104\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5558 - val_loss: 0.5068\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5522 - val_loss: 0.5032\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5486 - val_loss: 0.5002\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.5453 - val_loss: 0.4974\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5423 - val_loss: 0.4944\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5393 - val_loss: 0.4920\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5365 - val_loss: 0.4896\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5337 - val_loss: 0.4871\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 781us/step - loss: 0.5312 - val_loss: 0.4848\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5287 - val_loss: 0.4826\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.5264 - val_loss: 0.4808\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5242 - val_loss: 0.4789\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5221 - val_loss: 0.4771\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5201 - val_loss: 0.4753\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5182 - val_loss: 0.4738\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5163 - val_loss: 0.4724\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5145 - val_loss: 0.4707\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5129 - val_loss: 0.4693\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5113 - val_loss: 0.4683\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5097 - val_loss: 0.4671\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.5083 - val_loss: 0.4659\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5069 - val_loss: 0.4647\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.5056 - val_loss: 0.4633\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5043 - val_loss: 0.4620\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5029 - val_loss: 0.4611\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.5018 - val_loss: 0.4597\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.5007 - val_loss: 0.4589\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.4995 - val_loss: 0.4580\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4985 - val_loss: 0.4572\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.4975 - val_loss: 0.4563\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4964 - val_loss: 0.4555\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.4954 - val_loss: 0.4549\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.4945 - val_loss: 0.4538\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4935 - val_loss: 0.4534\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.4927 - val_loss: 0.4525\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4918 - val_loss: 0.4518\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4909 - val_loss: 0.4510\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.4900 - val_loss: 0.4504\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4893 - val_loss: 0.4497\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4884 - val_loss: 0.4490\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.4877 - val_loss: 0.4482\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.4868 - val_loss: 0.4478\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.4861 - val_loss: 0.4470\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4853 - val_loss: 0.4466\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.4846 - val_loss: 0.4459\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.4839 - val_loss: 0.4455\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.4831 - val_loss: 0.4449\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4825 - val_loss: 0.4440\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4818 - val_loss: 0.4436\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.4811 - val_loss: 0.4430\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.4804 - val_loss: 0.4422\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4797 - val_loss: 0.4418\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.4790 - val_loss: 0.4416\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4784 - val_loss: 0.4408\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.4777 - val_loss: 0.4407\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.4770 - val_loss: 0.4404\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4765 - val_loss: 0.4399\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.4758 - val_loss: 0.4392\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4751 - val_loss: 0.4386\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4745 - val_loss: 0.4384\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.4738 - val_loss: 0.4376\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.4733 - val_loss: 0.4370\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.4726 - val_loss: 0.4369\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.4721 - val_loss: 0.4361\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4715 - val_loss: 0.4361\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.4710 - val_loss: 0.4355\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.4703 - val_loss: 0.4353\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.4698 - val_loss: 0.4350\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.4666\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 6.9561 - val_loss: 5.3590\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 4.8731 - val_loss: 3.9202\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 3.5401 - val_loss: 2.9490\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 2.6609 - val_loss: 2.2800\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 2.0653 - val_loss: 1.8103\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 1.6527 - val_loss: 1.4764\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 724us/step - loss: 1.3627 - val_loss: 1.2363\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 1.1561 - val_loss: 1.0625\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 1.0078 - val_loss: 0.9359\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.9002 - val_loss: 0.8428\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.8216 - val_loss: 0.7742\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.7640 - val_loss: 0.7233\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.7216 - val_loss: 0.6855\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.6900 - val_loss: 0.6571\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.6665 - val_loss: 0.6356\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.6488 - val_loss: 0.6193\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.6354 - val_loss: 0.6067\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.6251 - val_loss: 0.5969\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.6171 - val_loss: 0.5892\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.6108 - val_loss: 0.5830\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6058 - val_loss: 0.5780\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.6017 - val_loss: 0.5739\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5984 - val_loss: 0.5704\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5955 - val_loss: 0.5674\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.5931 - val_loss: 0.5648\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5909 - val_loss: 0.5626\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.5890 - val_loss: 0.5605\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5872 - val_loss: 0.5587\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5856 - val_loss: 0.5569\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5842 - val_loss: 0.5553\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5828 - val_loss: 0.5538\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.5815 - val_loss: 0.5524\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.5802 - val_loss: 0.5511\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5790 - val_loss: 0.5498\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5778 - val_loss: 0.5486\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5767 - val_loss: 0.5474\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5756 - val_loss: 0.5462\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5746 - val_loss: 0.5451\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5736 - val_loss: 0.5440\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.5726 - val_loss: 0.5429\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.5716 - val_loss: 0.5419\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5707 - val_loss: 0.5409\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5698 - val_loss: 0.5400\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5689 - val_loss: 0.5390\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5680 - val_loss: 0.5381\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5672 - val_loss: 0.5372\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.5664 - val_loss: 0.5364\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5656 - val_loss: 0.5355\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5648 - val_loss: 0.5347\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5640 - val_loss: 0.5339\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5633 - val_loss: 0.5331\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.5626 - val_loss: 0.5323\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5618 - val_loss: 0.5315\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5611 - val_loss: 0.5308\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.5605 - val_loss: 0.5301\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5598 - val_loss: 0.5294\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5591 - val_loss: 0.5287\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5585 - val_loss: 0.5280\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.5579 - val_loss: 0.5274\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5573 - val_loss: 0.5267\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5567 - val_loss: 0.5261\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5561 - val_loss: 0.5254\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5556 - val_loss: 0.5249\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5551 - val_loss: 0.5242\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5545 - val_loss: 0.5237\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5540 - val_loss: 0.5231\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.5535 - val_loss: 0.5226\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5530 - val_loss: 0.5220\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5525 - val_loss: 0.5215\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.5520 - val_loss: 0.5210\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5516 - val_loss: 0.5205\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.5511 - val_loss: 0.5200\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.5507 - val_loss: 0.5195\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.5503 - val_loss: 0.5191\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5499 - val_loss: 0.5186\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5495 - val_loss: 0.5181\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5490 - val_loss: 0.5177\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5487 - val_loss: 0.5172\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5483 - val_loss: 0.5168\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5479 - val_loss: 0.5164\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.5476 - val_loss: 0.5160\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5472 - val_loss: 0.5156\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5468 - val_loss: 0.5152\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5465 - val_loss: 0.5148\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5462 - val_loss: 0.5145\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.5459 - val_loss: 0.5141\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5455 - val_loss: 0.5137\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5452 - val_loss: 0.5134\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.5449 - val_loss: 0.5131\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.5446 - val_loss: 0.5128\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.5444 - val_loss: 0.5124\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.5441 - val_loss: 0.5121\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.5438 - val_loss: 0.5118\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5435 - val_loss: 0.5115\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5433 - val_loss: 0.5112\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5430 - val_loss: 0.5108\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5428 - val_loss: 0.5106\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.5426 - val_loss: 0.5103\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.5423 - val_loss: 0.5100\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5421 - val_loss: 0.5098\n",
      "121/121 [==============================] - 0s 441us/step - loss: 0.5643\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 5.7577 - val_loss: 4.6645\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 4.1332 - val_loss: 3.4935\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 3.0812 - val_loss: 2.6946\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 2.3785 - val_loss: 2.1399\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 1.8973 - val_loss: 1.7487\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 1.5612 - val_loss: 1.4698\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 1.3230 - val_loss: 1.2684\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 1.1518 - val_loss: 1.1220\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 1.0277 - val_loss: 1.0146\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.9370 - val_loss: 0.9352\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.8702 - val_loss: 0.8759\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.8207 - val_loss: 0.8313\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.7837 - val_loss: 0.7973\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.7558 - val_loss: 0.7712\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.7346 - val_loss: 0.7507\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.7182 - val_loss: 0.7344\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.7054 - val_loss: 0.7213\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.6952 - val_loss: 0.7104\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.6870 - val_loss: 0.7013\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.6803 - val_loss: 0.6934\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6746 - val_loss: 0.6864\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.6697 - val_loss: 0.6802\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.6655 - val_loss: 0.6746\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.6616 - val_loss: 0.6694\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.6582 - val_loss: 0.6645\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.6550 - val_loss: 0.6599\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.6520 - val_loss: 0.6555\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.6492 - val_loss: 0.6514\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.6465 - val_loss: 0.6473\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.6440 - val_loss: 0.6435\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.6415 - val_loss: 0.6396\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.6392 - val_loss: 0.6360\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.6369 - val_loss: 0.6324\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.6347 - val_loss: 0.6289\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.6325 - val_loss: 0.6255\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.6304 - val_loss: 0.6222\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.6284 - val_loss: 0.6190\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.6265 - val_loss: 0.6158\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.6245 - val_loss: 0.6128\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.6226 - val_loss: 0.6097\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.6207 - val_loss: 0.6068\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.6189 - val_loss: 0.6039\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.6171 - val_loss: 0.6011\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.6154 - val_loss: 0.5983\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.6137 - val_loss: 0.5956\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.6121 - val_loss: 0.5930\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.6105 - val_loss: 0.5904\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.6089 - val_loss: 0.5879\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.6074 - val_loss: 0.5854\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.6059 - val_loss: 0.5830\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.6044 - val_loss: 0.5807\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.6030 - val_loss: 0.5784\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.6016 - val_loss: 0.5761\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.6002 - val_loss: 0.5739\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.5988 - val_loss: 0.5718\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5975 - val_loss: 0.5697\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5962 - val_loss: 0.5677\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.5950 - val_loss: 0.5657\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.5938 - val_loss: 0.5638\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.5926 - val_loss: 0.5619\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5914 - val_loss: 0.5600\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.5902 - val_loss: 0.5582\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.5891 - val_loss: 0.5565\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5880 - val_loss: 0.5547\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5869 - val_loss: 0.5531\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5859 - val_loss: 0.5515\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.5849 - val_loss: 0.5499\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.5839 - val_loss: 0.5483\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5829 - val_loss: 0.5468\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5819 - val_loss: 0.5454\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5810 - val_loss: 0.5439\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5801 - val_loss: 0.5425\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5792 - val_loss: 0.5412\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.5783 - val_loss: 0.5399\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.5774 - val_loss: 0.5386\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.5766 - val_loss: 0.5373\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.5757 - val_loss: 0.5361\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.5749 - val_loss: 0.5349\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.5741 - val_loss: 0.5338\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5733 - val_loss: 0.5327\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5726 - val_loss: 0.5316\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.5719 - val_loss: 0.5305\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.5711 - val_loss: 0.5295\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.5704 - val_loss: 0.5286\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.5697 - val_loss: 0.5276\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.5691 - val_loss: 0.5267\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5684 - val_loss: 0.5258\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5677 - val_loss: 0.5250\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5671 - val_loss: 0.5241\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5665 - val_loss: 0.5234\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.5659 - val_loss: 0.5226\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.5652 - val_loss: 0.5219\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.5647 - val_loss: 0.5212\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5641 - val_loss: 0.5204\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.5635 - val_loss: 0.5198\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.5629 - val_loss: 0.5192\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.5624 - val_loss: 0.5186\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.5619 - val_loss: 0.5181\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.5614 - val_loss: 0.5175\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.5609 - val_loss: 0.5170\n",
      "121/121 [==============================] - 0s 467us/step - loss: 0.5491\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 7.0315 - val_loss: 5.8152\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 675us/step - loss: 5.1325 - val_loss: 4.3096\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 3.8204 - val_loss: 3.2551\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 701us/step - loss: 2.9052 - val_loss: 2.5100\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 2.2624 - val_loss: 1.9810\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 1.8080 - val_loss: 1.6029\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 686us/step - loss: 1.4844 - val_loss: 1.3309\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 754us/step - loss: 1.2528 - val_loss: 1.1346\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 694us/step - loss: 1.0866 - val_loss: 0.9921\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.9666 - val_loss: 0.8883\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.8796 - val_loss: 0.8123\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.8162 - val_loss: 0.7563\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.7698 - val_loss: 0.7149\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.7356 - val_loss: 0.6840\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.7103 - val_loss: 0.6608\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.6912 - val_loss: 0.6431\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.6768 - val_loss: 0.6295\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.6658 - val_loss: 0.6188\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6571 - val_loss: 0.6104\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.6503 - val_loss: 0.6037\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.6447 - val_loss: 0.5981\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.6401 - val_loss: 0.5935\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.6363 - val_loss: 0.5895\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.6329 - val_loss: 0.5861\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6300 - val_loss: 0.5830\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.6273 - val_loss: 0.5802\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.6249 - val_loss: 0.5777\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.6226 - val_loss: 0.5754\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.6205 - val_loss: 0.5733\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.6186 - val_loss: 0.5712\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.6166 - val_loss: 0.5693\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6148 - val_loss: 0.5674\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.6131 - val_loss: 0.5657\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.6114 - val_loss: 0.5640\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6097 - val_loss: 0.5624\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.6081 - val_loss: 0.5608\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.6065 - val_loss: 0.5592\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.6050 - val_loss: 0.5577\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.6035 - val_loss: 0.5562\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.6021 - val_loss: 0.5548\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.6006 - val_loss: 0.5534\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.5992 - val_loss: 0.5521\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.5979 - val_loss: 0.5507\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5965 - val_loss: 0.5495\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.5952 - val_loss: 0.5482\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.5940 - val_loss: 0.5470\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.5927 - val_loss: 0.5457\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5915 - val_loss: 0.5446\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.5903 - val_loss: 0.5434\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.5891 - val_loss: 0.5423\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5880 - val_loss: 0.5413\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.5869 - val_loss: 0.5402\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 765us/step - loss: 0.5858 - val_loss: 0.5391\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.5847 - val_loss: 0.5381\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.5836 - val_loss: 0.5371\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.5826 - val_loss: 0.5361\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.5816 - val_loss: 0.5352\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.5806 - val_loss: 0.5343\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.5797 - val_loss: 0.5333\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.5787 - val_loss: 0.5324\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5778 - val_loss: 0.5316\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.5769 - val_loss: 0.5307\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.5760 - val_loss: 0.5299\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.5751 - val_loss: 0.5291\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.5742 - val_loss: 0.5282\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.5734 - val_loss: 0.5275\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.5726 - val_loss: 0.5267\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.5718 - val_loss: 0.5259\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5710 - val_loss: 0.5252\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.5703 - val_loss: 0.5245\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.5695 - val_loss: 0.5238\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.5688 - val_loss: 0.5231\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.5681 - val_loss: 0.5224\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.5674 - val_loss: 0.5217\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.5667 - val_loss: 0.5211\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.5660 - val_loss: 0.5205\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.5654 - val_loss: 0.5199\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5647 - val_loss: 0.5193\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5641 - val_loss: 0.5187\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.5635 - val_loss: 0.5181\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.5629 - val_loss: 0.5175\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.5623 - val_loss: 0.5170\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.5617 - val_loss: 0.5164\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5611 - val_loss: 0.5159\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.5606 - val_loss: 0.5154\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.5600 - val_loss: 0.5149\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.5595 - val_loss: 0.5144\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.5590 - val_loss: 0.5139\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.5585 - val_loss: 0.5134\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.5580 - val_loss: 0.5130\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.5575 - val_loss: 0.5125\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.5570 - val_loss: 0.5120\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.5565 - val_loss: 0.5116\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5561 - val_loss: 0.5112\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.5556 - val_loss: 0.5107\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.5552 - val_loss: 0.5104\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.5547 - val_loss: 0.5099\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5543 - val_loss: 0.5096\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.5539 - val_loss: 0.5092\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.5535 - val_loss: 0.5088\n",
      "121/121 [==============================] - 0s 484us/step - loss: 0.5500\n",
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 47s - loss: 6.6430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdriánPortilloSánche\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [        nan -0.43224024         nan -0.54863852         nan -0.45666197\n",
      " -0.32936981 -1.34245129 -0.54342322 -0.55447674]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 804us/step - loss: 1.0507 - val_loss: 0.5857\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.6277 - val_loss: 0.5158\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.5471 - val_loss: 0.4373\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.4629 - val_loss: 0.4099\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 651us/step - loss: 0.4413 - val_loss: 0.4022\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.4265 - val_loss: 0.3906\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.4186 - val_loss: 0.3806\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.4109 - val_loss: 0.3778\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.4016 - val_loss: 0.3718\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 643us/step - loss: 0.3939 - val_loss: 0.3711\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3887 - val_loss: 0.3659\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.3829 - val_loss: 0.3711\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3781 - val_loss: 0.3662\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3718 - val_loss: 0.3545\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.3687 - val_loss: 0.3557\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.3642 - val_loss: 0.3498\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.3604 - val_loss: 0.3469\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 651us/step - loss: 0.3558 - val_loss: 0.3467\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.3531 - val_loss: 0.3417\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3497 - val_loss: 0.3411\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.3470 - val_loss: 0.3368\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 678us/step - loss: 0.3433 - val_loss: 0.3350\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.3415 - val_loss: 0.3314\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 662us/step - loss: 0.3390 - val_loss: 0.3310\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3356 - val_loss: 0.3299\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 670us/step - loss: 0.3336 - val_loss: 0.3252\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.3320 - val_loss: 0.3257\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.3304 - val_loss: 0.3222\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3270 - val_loss: 0.3254\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3231 - val_loss: 0.3214\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.3225 - val_loss: 0.3233\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.3208 - val_loss: 0.3194\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3202 - val_loss: 0.3180\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 678us/step - loss: 0.3180 - val_loss: 0.3161\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.3167 - val_loss: 0.3205\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 677us/step - loss: 0.3134 - val_loss: 0.3105\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 678us/step - loss: 0.3107 - val_loss: 0.3098\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.3083 - val_loss: 0.3182\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3070 - val_loss: 0.3060\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.3044 - val_loss: 0.3075\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 670us/step - loss: 0.3032 - val_loss: 0.3129\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.3021 - val_loss: 0.3093\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 668us/step - loss: 0.3003 - val_loss: 0.3063\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.2993 - val_loss: 0.3031\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.2972 - val_loss: 0.3019\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.2967 - val_loss: 0.3039\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.2950 - val_loss: 0.3004\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.2952 - val_loss: 0.2982\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.2942 - val_loss: 0.2986\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 662us/step - loss: 0.2954 - val_loss: 0.2964\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.2924 - val_loss: 0.3012\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.2922 - val_loss: 0.2978\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.2895 - val_loss: 0.3038\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.2899 - val_loss: 0.3082\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.2904 - val_loss: 0.2970\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.2889 - val_loss: 0.2984\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.2884 - val_loss: 0.3028\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.2886 - val_loss: 0.3022\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.2875 - val_loss: 0.2950\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.2871 - val_loss: 0.2942\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.2892 - val_loss: 0.2941\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.2855 - val_loss: 0.2977\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.2852 - val_loss: 0.2964\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.2909 - val_loss: 0.2909\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.2840 - val_loss: 0.2922\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 672us/step - loss: 0.2834 - val_loss: 0.2900\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 658us/step - loss: 0.2833 - val_loss: 0.2946\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 673us/step - loss: 0.2809 - val_loss: 0.2894\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 674us/step - loss: 0.2817 - val_loss: 0.2884\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.2818 - val_loss: 0.2970\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.2812 - val_loss: 0.2883\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.2797 - val_loss: 0.2942\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.2781 - val_loss: 0.2934\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.2780 - val_loss: 0.2881\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.2768 - val_loss: 0.2914\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.2766 - val_loss: 0.2891\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.2762 - val_loss: 0.2871\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 652us/step - loss: 0.2755 - val_loss: 0.2845\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 657us/step - loss: 0.2753 - val_loss: 0.2867\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.2753 - val_loss: 0.2877\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 655us/step - loss: 0.2736 - val_loss: 0.2959\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 660us/step - loss: 0.2730 - val_loss: 0.2883\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.2729 - val_loss: 0.2892\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.2725 - val_loss: 0.2879\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.2733 - val_loss: 0.2852\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.2722 - val_loss: 0.2870\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 686us/step - loss: 0.2712 - val_loss: 0.2903\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 701us/step - loss: 0.2720 - val_loss: 0.2850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001ABA6EE07C0&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001ABA6ECAAA0&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001ABA6EE07C0&gt;,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001ABA6ECAAA0&gt;,\n",
       "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
       "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001ABA6EE07C0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x000001ABA6EE07C0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001ABA6EE07C0>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x000001ABA6ECAAA0>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_distribs = {\n",
    " \"n_hidden\": [0, 1, 2, 3],\n",
    " \"n_neurons\": np.arange(1, 100),\n",
    " \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exploración puede ocupar varias horas, una vez que se haya completado, podemos ver los mejores parámetros encontrados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.004683082198532954, 'n_hidden': 2, 'n_neurons': 62}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.32936981320381165"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos guardar el modelo, evaluarlo en el conjunto de pruebas, y si estamos satisfechos con el rendimiento, desplegarlo en producción.\n",
    "\n",
    "Ya que el proceso de encontrar parámetros es costoso se ofrecen diferentes técnicas para encontrar soluciones en menos tiempo, algunas librerías de Python para optimizar hiperparámetros son:\n",
    "\n",
    "* Hyperopt: Para optimizar hiperparámetros de todo tipo de espacios de búsqueda\n",
    "* Hyperas, kopt y talos: Para optimizar hiperparámetros de modelos de Keras\n",
    "* Keras Tuner: Realizada por Google, para modelos de Keras\n",
    "* SciKit-Optimize (Skopt): Librería de optimización de propósito general, la clase BayesSearchCV es una implementación de búsqueda bayesiana\n",
    "* Spearmint: Librería de optimización bayesiana.\n",
    "* Hyperband: Una librería de gran velocidad basada en el algoritmo Hyperband.\n",
    "* Sklearn-Deap: Una librería basada en algoritmos genéticos.\n",
    "\n",
    "Además, muchas compañías ofrecen servicios de optimización de hiperparámetros, como por ejemplo:\n",
    "\n",
    "* Amazon SageMaker.\n",
    "* Google Cloud AI Platform.\n",
    "* Microsoft Azure Machine Learning.\n",
    "* Paperspace Gradient.\n",
    "* Databricks.\n",
    "* H2O Driverless AI.\n",
    "* Domino Data Lab.\n",
    "* DataRobot.\n",
    "* SigOpt.\n",
    "* Tune.\n",
    "* Optuna.\n",
    "* Weights & Biases.\n",
    "\n",
    "### Número de capas ocultas\n",
    "\n",
    "El número de capas ocultas es un hiperparámetro importante. Si el modelo tiene demasiadas capas, puede tener problemas de sobreajuste, mientras que si tiene demasiadas capas, puede tener problemas de subajuste.\n",
    "\n",
    "Para encontrar el número óptimo de capas ocultas, podemos entrenar el modelo con diferentes números de capas ocultas, y ver cuál obtiene el mejor rendimiento en el conjunto de validación. Por ejemplo, podemos entrenar el modelo con 1, 2, 3, 4, 5, 6, 7, 8, 9 y 10 capas ocultas, y ver cuál obtiene el mejor rendimiento en el conjunto de validación.\n",
    "\n",
    "### Número de neuronas por capa\n",
    "\n",
    "El número de neuronas por capa es otro hiperparámetro importante. Si el modelo tiene demasiadas neuronas por capa, puede tener problemas de sobreajuste, mientras que si tiene demasiadas neuronas por capa, puede tener problemas de subajuste.\n",
    "\n",
    "Para encontrar el número óptimo de neuronas por capa, podemos entrenar el modelo con diferentes números de neuronas por capa, y ver cuál obtiene el mejor rendimiento en el conjunto de validación. Por ejemplo, podemos entrenar el modelo con 10, 20, 30, 40, 50, 60, 70, 80, 90 y 100 neuronas por capa, y ver cuál obtiene el mejor rendimiento en el conjunto de validación.\n",
    "\n",
    "### Tasa de aprendizaje\n",
    "\n",
    "La tasa de aprendizaje es el hiperparámetro mñas importante. Si la tasa de aprendizaje es demasiado alta, el modelo puede diverger, mientras que si es demasiado baja, el modelo puede converger demasiado lentamente.\n",
    "\n",
    "Una buena manera es empezar con una tasa de aprendizaje muy baja, y luego ir aumentándola gradualmente hasta que el modelo empiece a diverger. Por ejemplo, podemos empezar con 0.0001, luego 0.001, luego 0.01, luego 0.1, luego 1.0, luego 10.0, luego 100.0, y luego 1000.0. Si el modelo diverge con 0.0001, entonces 0.001 es demasiado baja, y si el modelo diverge con 1.0, entonces 10.0 es demasiado alta.\n",
    "\n",
    "Esto se consigue multiplicando la tasa por un factor constante en cada iteración, si mostramos la pérdida en función de la tasa de aprendizaje, podemos ver que la pérdida empieza cayendo, pero cuando la tasa de aprendizaje es demasiado alta, la pérdida empieza a subir, el punto en el que la pérdida empieza a subir es la tasa de aprendizaje óptima.\t\n",
    "\n",
    "### Optimizador\n",
    "\n",
    "El optimizador es también muy importante, veremos optimizadores más avanzados posteriormente.\n",
    "\n",
    "### Tamaño del lote\n",
    "\n",
    "El tamaño del lote tendrá un gran impacto en el rendimiento del modelo y el tiempo de entrenamiento. \n",
    "\n",
    "Si usamos grandes lotes aceleradores de hardware como GPU o TPU pueden procesarlos más rápido. Se recomienda usar el tamaño de lote que quepa en la memoria RAM de la GPU o TPU.\n",
    "\n",
    "Sin embargo, los tamaños grandes de lote pueden dar lugar a inestabilidad en el entrenamiento, por lo que podemos usar un tamaño de lote grande, usando una tasa de aprendizaje baja, y si da lugar a resultados inestables, entonces podemos usar un tamaño de lote más pequeño.\n",
    "\n",
    "### Función de activación\n",
    "\n",
    "La función de activación es un hiperparámetro importante, y depende de la tarea. Por ejemplo, para la tarea de clasificación de imágenes, la función de activación de la última capa suele ser la función de activación softmax, mientras que para la tarea de regresión, la función de activación de la última capa suele ser la función de activación lineal.\n",
    "\n",
    "Para las capas ocultas una buena función de activación por defecto sería ReLU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca2906c06b6a54c9cc575d13f9faa244526440c398aabf161b771b68f1e356a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
